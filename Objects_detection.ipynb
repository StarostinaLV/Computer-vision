{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfBsZPPV2rcwqgtFE6dV63"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**«Компьютерное зрение»**\n",
        "\n",
        "Решить задачу детекции на основе SSD для [датасета](https://github.com/Shenggan/BCCD_Dataset).\n",
        "\n",
        "Реализацию SSD можно подглядеть [тут](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)."
      ],
      "metadata": {
        "id": "NYZaHTCzM-i5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Клонирование файлов датасета BCCD и файлов с примером реализации SSD."
      ],
      "metadata": {
        "id": "zNK1zYDOIc3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Shenggan/BCCD_Dataset.git\n",
        "!git clone https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z-7IS9Il1r3",
        "outputId": "bafd3355-6d16-47aa-e596-d68b547fa828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BCCD_Dataset'...\n",
            "remote: Enumerating objects: 800, done.\u001b[K\n",
            "remote: Total 800 (delta 0), reused 0 (delta 0), pack-reused 800 (from 1)\u001b[K\n",
            "Receiving objects: 100% (800/800), 7.39 MiB | 22.45 MiB/s, done.\n",
            "Resolving deltas: 100% (378/378), done.\n",
            "Cloning into 'a-PyTorch-Tutorial-to-Object-Detection'...\n",
            "remote: Enumerating objects: 269, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 269 (delta 72), reused 64 (delta 58), pack-reused 181 (from 1)\u001b[K\n",
            "Receiving objects: 100% (269/269), 175.98 MiB | 31.74 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming directory \"a-PyTorch-Tutorial-to-Object-Detection\" to get rid of hyphens\n",
        "import os\n",
        "\n",
        "os.rename(r'./a-PyTorch-Tutorial-to-Object-Detection', r'./a_PyTorch_Tutorial_to_Object_Detection')"
      ],
      "metadata": {
        "id": "A9nB6fJfnRQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "import torchvision.transforms.functional as FT"
      ],
      "metadata": {
        "id": "vyi-TKBo2OeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Карта меток для трех классов ('rbc', 'wbc', 'platelets')."
      ],
      "metadata": {
        "id": "GSGjpRWIJCev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Label map\n",
        "voc_labels = ('rbc', 'wbc', 'platelets')\n",
        "label_map = {k: v + 1 for v, k in enumerate(voc_labels)}\n",
        "label_map['background'] = 0\n",
        "rev_label_map = {v: k for k, v in label_map.items()}  # Inverse mapping\n",
        "\n",
        "# Color map for bounding boxes of detected objects from https://sashat.me/2017/01/11/list-of-20-simple-distinct-colors/\n",
        "distinct_colors = ['#e6194b', '#3cb44b', '#ffe119', '#0082c8']\n",
        "label_color_map = {k: distinct_colors[i] for i, k in enumerate(label_map.keys())}"
      ],
      "metadata": {
        "id": "B31Vi4mUtB0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Парсинг файлов с аннотациями."
      ],
      "metadata": {
        "id": "kJ6cgA2FJQLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_annotation(annotation_path): # BCCD_Dataset/BCCD/Annotations\n",
        "    tree = ET.parse(annotation_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    boxes = list()\n",
        "    labels = list()\n",
        "    difficulties = list()\n",
        "    for object in root.iter('object'):\n",
        "\n",
        "        difficult = int(object.find('difficult').text == '1')\n",
        "\n",
        "        label = object.find('name').text.lower().strip()\n",
        "        if label not in label_map:\n",
        "            continue\n",
        "\n",
        "        bbox = object.find('bndbox')\n",
        "        xmin = int(bbox.find('xmin').text) - 1\n",
        "        ymin = int(bbox.find('ymin').text) - 1\n",
        "        xmax = int(bbox.find('xmax').text) - 1\n",
        "        ymax = int(bbox.find('ymax').text) - 1\n",
        "\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(label_map[label])\n",
        "        difficulties.append(difficult)\n",
        "\n",
        "    return {'boxes': boxes, 'labels': labels, 'difficulties': difficulties}"
      ],
      "metadata": {
        "id": "HAT5lRRA7OuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создрание json-файлов с информацией о путях к файлам с изображениями и об объектах на них для обучающей и тестовой выборок."
      ],
      "metadata": {
        "id": "_MmJwdTCJZaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_lists(dataset_path, output_folder):\n",
        "    \"\"\"\n",
        "    Create lists of images, the bounding boxes and labels of the objects in these images, and save these to file.\n",
        "\n",
        "    :param path: path to the 'BCCD' folder\n",
        "    :param output_folder: folder where the JSONs must be saved\n",
        "    \"\"\"\n",
        "    path = os.path.abspath(dataset_path)\n",
        "\n",
        "    train_images = list()\n",
        "    train_objects = list()\n",
        "    n_objects = 0\n",
        "\n",
        "    # Training data\n",
        "\n",
        "    # Find IDs of images in training data\n",
        "    with open(os.path.join(path, 'ImageSets/Main/trainval.txt')) as f:\n",
        "        ids = f.read().splitlines()\n",
        "\n",
        "    for id in ids:\n",
        "        # Parse annotation's XML file\n",
        "        objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
        "        if len(objects) == 0:\n",
        "            continue\n",
        "        n_objects += len(objects['boxes'])\n",
        "        train_objects.append(objects)\n",
        "        train_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
        "\n",
        "    assert len(train_objects) == len(train_images)\n",
        "\n",
        "    # Save to file\n",
        "    with open(os.path.join(output_folder, 'TRAIN_images.json'), 'w') as j:\n",
        "        json.dump(train_images, j)\n",
        "    with open(os.path.join(output_folder, 'TRAIN_objects.json'), 'w') as j:\n",
        "        json.dump(train_objects, j)\n",
        "    with open(os.path.join(output_folder, 'label_map.json'), 'w') as j:\n",
        "        json.dump(label_map, j)  # save label map too\n",
        "\n",
        "    print('\\nThere are %d training images containing a total of %d objects. Files have been saved to %s.' % (\n",
        "        len(train_images), n_objects, os.path.abspath(output_folder)))\n",
        "\n",
        "    # Test data\n",
        "    test_images = list()\n",
        "    test_objects = list()\n",
        "    n_objects = 0\n",
        "\n",
        "    # Find IDs of images in the test data\n",
        "    with open(os.path.join(path, 'ImageSets/Main/test.txt')) as f:\n",
        "        ids = f.read().splitlines()\n",
        "\n",
        "    for id in ids:\n",
        "        # Parse annotation's XML file\n",
        "        objects = parse_annotation(os.path.join(path, 'Annotations', id + '.xml'))\n",
        "        if len(objects) == 0:\n",
        "            continue\n",
        "        test_objects.append(objects)\n",
        "        n_objects += len(objects['boxes'])\n",
        "        test_images.append(os.path.join(path, 'JPEGImages', id + '.jpg'))\n",
        "\n",
        "    assert len(test_objects) == len(test_images)\n",
        "\n",
        "    # Save to file\n",
        "    with open(os.path.join(output_folder, 'TEST_images.json'), 'w') as j:\n",
        "        json.dump(test_images, j)\n",
        "    with open(os.path.join(output_folder, 'TEST_objects.json'), 'w') as j:\n",
        "        json.dump(test_objects, j)\n",
        "\n",
        "    print('\\nThere are %d test images containing a total of %d objects. Files have been saved to %s.' % (\n",
        "        len(test_images), n_objects, os.path.abspath(output_folder)))"
      ],
      "metadata": {
        "id": "eoAO31QOvXBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_data_lists('BCCD_Dataset/BCCD', 'BCCD_Dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb3rGlgD3LTt",
        "outputId": "07b3b467-13e3-437c-d987-b2321496120a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "There are 292 training images containing a total of 3943 objects. Files have been saved to /content/BCCD_Dataset.\n",
            "\n",
            "There are 72 test images containing a total of 945 objects. Files have been saved to /content/BCCD_Dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformations of the images."
      ],
      "metadata": {
        "id": "jnYVWcXAAdx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применяемые к изображениям трансформации:"
      ],
      "metadata": {
        "id": "X1cjnk1fLC6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_intersection(set_1, set_2):\n",
        "    \"\"\"\n",
        "    Find the intersection of every box combination between two sets of boxes that are in boundary coordinates.\n",
        "\n",
        "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
        "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
        "    :return: intersection of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
        "    \"\"\"\n",
        "\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    lower_bounds = torch.max(set_1[:, :2].unsqueeze(1), set_2[:, :2].unsqueeze(0))  # (n1, n2, 2)\n",
        "    upper_bounds = torch.min(set_1[:, 2:].unsqueeze(1), set_2[:, 2:].unsqueeze(0))  # (n1, n2, 2)\n",
        "    intersection_dims = torch.clamp(upper_bounds - lower_bounds, min=0)  # (n1, n2, 2)\n",
        "    return intersection_dims[:, :, 0] * intersection_dims[:, :, 1]  # (n1, n2)\n",
        "\n",
        "def find_jaccard_overlap(set_1, set_2):\n",
        "    \"\"\"\n",
        "    Find the Jaccard Overlap (IoU) of every box combination between two sets of boxes that are in boundary coordinates.\n",
        "\n",
        "    :param set_1: set 1, a tensor of dimensions (n1, 4)\n",
        "    :param set_2: set 2, a tensor of dimensions (n2, 4)\n",
        "    :return: Jaccard Overlap of each of the boxes in set 1 with respect to each of the boxes in set 2, a tensor of dimensions (n1, n2)\n",
        "    \"\"\"\n",
        "\n",
        "    # Find intersections\n",
        "    intersection = find_intersection(set_1, set_2)  # (n1, n2)\n",
        "\n",
        "    # Find areas of each box in both sets\n",
        "    areas_set_1 = (set_1[:, 2] - set_1[:, 0]) * (set_1[:, 3] - set_1[:, 1])  # (n1)\n",
        "    areas_set_2 = (set_2[:, 2] - set_2[:, 0]) * (set_2[:, 3] - set_2[:, 1])  # (n2)\n",
        "\n",
        "    # Find the union\n",
        "    # PyTorch auto-broadcasts singleton dimensions\n",
        "    union = areas_set_1.unsqueeze(1) + areas_set_2.unsqueeze(0) - intersection  # (n1, n2)\n",
        "\n",
        "    return intersection / union  # (n1, n2)\n",
        "\n",
        "def expand(image, boxes, filler):\n",
        "    \"\"\"\n",
        "    Perform a zooming out operation by placing the image in a larger canvas of filler material.\n",
        "\n",
        "    Helps to learn to detect smaller objects.\n",
        "\n",
        "    :param image: image, a tensor of dimensions (3, original_h, original_w)\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :param filler: RBG values of the filler material, a list like [R, G, B]\n",
        "    :return: expanded image, updated bounding box coordinates\n",
        "    \"\"\"\n",
        "    # Calculate dimensions of proposed expanded (zoomed-out) image\n",
        "    original_h = image.size(1)\n",
        "    original_w = image.size(2)\n",
        "    max_scale = 4\n",
        "    scale = random.uniform(1, max_scale)\n",
        "    new_h = int(scale * original_h)\n",
        "    new_w = int(scale * original_w)\n",
        "\n",
        "    # Create such an image with the filler\n",
        "    filler = torch.FloatTensor(filler)  # (3)\n",
        "    new_image = torch.ones((3, new_h, new_w), dtype=torch.float) * filler.unsqueeze(1).unsqueeze(1)  # (3, new_h, new_w)\n",
        "    # Note - do not use expand() like new_image = filler.unsqueeze(1).unsqueeze(1).expand(3, new_h, new_w)\n",
        "    # because all expanded values will share the same memory, so changing one pixel will change all\n",
        "\n",
        "    # Place the original image at random coordinates in this new image (origin at top-left of image)\n",
        "    left = random.randint(0, new_w - original_w)\n",
        "    right = left + original_w\n",
        "    top = random.randint(0, new_h - original_h)\n",
        "    bottom = top + original_h\n",
        "    new_image[:, top:bottom, left:right] = image\n",
        "\n",
        "    # Adjust bounding boxes' coordinates accordingly\n",
        "    new_boxes = boxes + torch.FloatTensor([left, top, left, top]).unsqueeze(\n",
        "        0)  # (n_objects, 4), n_objects is the no. of objects in this image\n",
        "\n",
        "    return new_image, new_boxes\n",
        "\n",
        "\n",
        "def random_crop(image, boxes, labels, difficulties):\n",
        "    \"\"\"\n",
        "    Performs a random crop in the manner stated in the paper. Helps to learn to detect larger and partial objects.\n",
        "\n",
        "    Note that some objects may be cut out entirely.\n",
        "\n",
        "    Adapted from https://github.com/amdegroot/ssd.pytorch/blob/master/utils/augmentations.py\n",
        "\n",
        "    :param image: image, a tensor of dimensions (3, original_h, original_w)\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
        "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
        "    :return: cropped image, updated bounding box coordinates, updated labels, updated difficulties\n",
        "    \"\"\"\n",
        "    original_h = image.size(1)\n",
        "    original_w = image.size(2)\n",
        "    # Keep choosing a minimum overlap until a successful crop is made\n",
        "    while True:\n",
        "        # Randomly draw the value for minimum overlap\n",
        "        min_overlap = random.choice([0., .1, .3, .5, .7, .9, None])  # 'None' refers to no cropping\n",
        "\n",
        "        # If not cropping\n",
        "        if min_overlap is None:\n",
        "            return image, boxes, labels, difficulties\n",
        "\n",
        "        # Try up to 50 times for this choice of minimum overlap\n",
        "        # This isn't mentioned in the paper, of course, but 50 is chosen in paper authors' original Caffe repo\n",
        "        max_trials = 50\n",
        "        for _ in range(max_trials):\n",
        "            # Crop dimensions must be in [0.3, 1] of original dimensions\n",
        "            # Note - it's [0.1, 1] in the paper, but actually [0.3, 1] in the authors' repo\n",
        "            min_scale = 0.3\n",
        "            scale_h = random.uniform(min_scale, 1)\n",
        "            scale_w = random.uniform(min_scale, 1)\n",
        "            new_h = int(scale_h * original_h)\n",
        "            new_w = int(scale_w * original_w)\n",
        "\n",
        "            # Aspect ratio has to be in [0.5, 2]\n",
        "            aspect_ratio = new_h / new_w\n",
        "            if not 0.5 < aspect_ratio < 2:\n",
        "                continue\n",
        "\n",
        "            # Crop coordinates (origin at top-left of image)\n",
        "            left = random.randint(0, original_w - new_w)\n",
        "            right = left + new_w\n",
        "            top = random.randint(0, original_h - new_h)\n",
        "            bottom = top + new_h\n",
        "            crop = torch.FloatTensor([left, top, right, bottom])  # (4)\n",
        "\n",
        "            # Calculate Jaccard overlap between the crop and the bounding boxes\n",
        "            overlap = find_jaccard_overlap(crop.unsqueeze(0),\n",
        "                                           boxes)  # (1, n_objects), n_objects is the no. of objects in this image\n",
        "            overlap = overlap.squeeze(0)  # (n_objects)\n",
        "\n",
        "            # If not a single bounding box has a Jaccard overlap of greater than the minimum, try again\n",
        "            if overlap.max().item() < min_overlap:\n",
        "                continue\n",
        "\n",
        "            # Crop image\n",
        "            new_image = image[:, top:bottom, left:right]  # (3, new_h, new_w)\n",
        "\n",
        "            # Find centers of original bounding boxes\n",
        "            bb_centers = (boxes[:, :2] + boxes[:, 2:]) / 2.  # (n_objects, 2)\n",
        "\n",
        "            # Find bounding boxes whose centers are in the crop\n",
        "            centers_in_crop = (bb_centers[:, 0] > left) * (bb_centers[:, 0] < right) * (bb_centers[:, 1] > top) * (\n",
        "                    bb_centers[:, 1] < bottom)  # (n_objects), a Torch uInt8/Byte tensor, can be used as a boolean index\n",
        "\n",
        "            # If not a single bounding box has its center in the crop, try again\n",
        "            if not centers_in_crop.any():\n",
        "                continue\n",
        "\n",
        "            # Discard bounding boxes that don't meet this criterion\n",
        "            new_boxes = boxes[centers_in_crop, :]\n",
        "            new_labels = labels[centers_in_crop]\n",
        "            new_difficulties = difficulties[centers_in_crop]\n",
        "\n",
        "            # Calculate bounding boxes' new coordinates in the crop\n",
        "            new_boxes[:, :2] = torch.max(new_boxes[:, :2], crop[:2])  # crop[:2] is [left, top]\n",
        "            new_boxes[:, :2] -= crop[:2]\n",
        "            new_boxes[:, 2:] = torch.min(new_boxes[:, 2:], crop[2:])  # crop[2:] is [right, bottom]\n",
        "            new_boxes[:, 2:] -= crop[:2]\n",
        "\n",
        "            return new_image, new_boxes, new_labels, new_difficulties\n",
        "\n",
        "\n",
        "def flip(image, boxes):\n",
        "    \"\"\"\n",
        "    Flip image horizontally.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :return: flipped image, updated bounding box coordinates\n",
        "    \"\"\"\n",
        "    # Flip image\n",
        "    new_image = FT.hflip(image)\n",
        "\n",
        "    # Flip boxes\n",
        "    new_boxes = boxes\n",
        "    new_boxes[:, 0] = image.width - boxes[:, 0] - 1\n",
        "    new_boxes[:, 2] = image.width - boxes[:, 2] - 1\n",
        "    new_boxes = new_boxes[:, [2, 1, 0, 3]]\n",
        "\n",
        "    return new_image, new_boxes\n",
        "\n",
        "\n",
        "def resize(image, boxes, dims=(300, 300), return_percent_coords=True):\n",
        "    \"\"\"\n",
        "    Resize image. For the SSD300, resize to (300, 300).\n",
        "\n",
        "    Since percent/fractional coordinates are calculated for the bounding boxes (w.r.t image dimensions) in this process,\n",
        "    you may choose to retain them.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :return: resized image, updated bounding box coordinates (or fractional coordinates, in which case they remain the same)\n",
        "    \"\"\"\n",
        "    # Resize image\n",
        "    new_image = FT.resize(image, dims)\n",
        "\n",
        "    # Resize bounding boxes\n",
        "    old_dims = torch.FloatTensor([image.width, image.height, image.width, image.height]).unsqueeze(0)\n",
        "    new_boxes = boxes / old_dims  # percent coordinates\n",
        "\n",
        "    if not return_percent_coords:\n",
        "        new_dims = torch.FloatTensor([dims[1], dims[0], dims[1], dims[0]]).unsqueeze(0)\n",
        "        new_boxes = new_boxes * new_dims\n",
        "\n",
        "    return new_image, new_boxes\n",
        "\n",
        "\n",
        "def photometric_distort(image):\n",
        "    \"\"\"\n",
        "    Distort brightness, contrast, saturation, and hue, each with a 50% chance, in random order.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :return: distorted image\n",
        "    \"\"\"\n",
        "    new_image = image\n",
        "\n",
        "    distortions = [FT.adjust_brightness,\n",
        "                   FT.adjust_contrast,\n",
        "                   FT.adjust_saturation,\n",
        "                   FT.adjust_hue]\n",
        "\n",
        "    random.shuffle(distortions)\n",
        "\n",
        "    for d in distortions:\n",
        "        if random.random() < 0.5:\n",
        "            if d.__name__ is 'adjust_hue':\n",
        "                # Caffe repo uses a 'hue_delta' of 18 - we divide by 255 because PyTorch needs a normalized value\n",
        "                adjust_factor = random.uniform(-18 / 255., 18 / 255.)\n",
        "            else:\n",
        "                # Caffe repo uses 'lower' and 'upper' values of 0.5 and 1.5 for brightness, contrast, and saturation\n",
        "                adjust_factor = random.uniform(0.5, 1.5)\n",
        "\n",
        "            # Apply this distortion\n",
        "            new_image = d(new_image, adjust_factor)\n",
        "\n",
        "    return new_image\n",
        "\n",
        "def transform(image, boxes, labels, difficulties, split):\n",
        "    \"\"\"\n",
        "    Apply the transformations above.\n",
        "\n",
        "    :param image: image, a PIL Image\n",
        "    :param boxes: bounding boxes in boundary coordinates, a tensor of dimensions (n_objects, 4)\n",
        "    :param labels: labels of objects, a tensor of dimensions (n_objects)\n",
        "    :param difficulties: difficulties of detection of these objects, a tensor of dimensions (n_objects)\n",
        "    :param split: one of 'TRAIN' or 'TEST', since different sets of transformations are applied\n",
        "    :return: transformed image, transformed bounding box coordinates, transformed labels, transformed difficulties\n",
        "    \"\"\"\n",
        "    assert split in {'TRAIN', 'TEST'}\n",
        "\n",
        "    # Mean and standard deviation of ImageNet data that our base VGG from torchvision was trained on\n",
        "    # see: https://pytorch.org/docs/stable/torchvision/models.html\n",
        "    mean = [0.485, 0.456, 0.406]\n",
        "    std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    new_image = image\n",
        "    new_boxes = boxes\n",
        "    new_labels = labels\n",
        "    new_difficulties = difficulties\n",
        "    # Skip the following operations for evaluation/testing\n",
        "    if split == 'TRAIN':\n",
        "        # A series of photometric distortions in random order, each with 50% chance of occurrence, as in Caffe repo\n",
        "        new_image = photometric_distort(new_image)\n",
        "\n",
        "        # Convert PIL image to Torch tensor\n",
        "        new_image = FT.to_tensor(new_image)\n",
        "\n",
        "        # Expand image (zoom out) with a 50% chance - helpful for training detection of small objects\n",
        "        # Fill surrounding space with the mean of ImageNet data that our base VGG was trained on\n",
        "        if random.random() < 0.5:\n",
        "            new_image, new_boxes = expand(new_image, boxes, filler=mean)\n",
        "\n",
        "        # Randomly crop image (zoom in)\n",
        "        new_image, new_boxes, new_labels, new_difficulties = random_crop(new_image, new_boxes, new_labels,\n",
        "                                                                         new_difficulties)\n",
        "\n",
        "        # Convert Torch tensor to PIL image\n",
        "        new_image = FT.to_pil_image(new_image)\n",
        "\n",
        "        # Flip image with a 50% chance\n",
        "        if random.random() < 0.5:\n",
        "            new_image, new_boxes = flip(new_image, new_boxes)\n",
        "\n",
        "    # Resize image to (300, 300) - this also converts absolute boundary coordinates to their fractional form\n",
        "    new_image, new_boxes = resize(new_image, new_boxes, dims=(300, 300))\n",
        "\n",
        "    # Convert PIL image to Torch tensor\n",
        "    new_image = FT.to_tensor(new_image)\n",
        "\n",
        "    # Normalize by mean and standard deviation of ImageNet data that our base VGG was trained on\n",
        "    new_image = FT.normalize(new_image, mean=mean, std=std)\n",
        "\n",
        "    return new_image, new_boxes, new_labels, new_difficulties"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03sgMnM6eD2T",
        "outputId": "a658e0f4-52c7-4c3b-a753-1ae45bf56685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:226: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<>:226: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "<ipython-input-8-891608adc205>:226: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if d.__name__ is 'adjust_hue':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BCCD_Dataset."
      ],
      "metadata": {
        "id": "qbOPzDljAm7R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс BCCD_Dataset для использования даталоудером."
      ],
      "metadata": {
        "id": "KPggYX4wL0LE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "# import sys\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class BCCD_Dataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, split, keep_difficult=False):\n",
        "        \"\"\"\n",
        "        :param data_folder: folder where data files are stored\n",
        "        :param split: split, one of 'TRAIN' or 'TEST'\n",
        "        :param keep_difficult: keep or discard objects that are considered difficult to detect?\n",
        "        \"\"\"\n",
        "        self.split = split.upper()\n",
        "\n",
        "        assert self.split in {'TRAIN', 'TEST'}\n",
        "\n",
        "        self.data_folder = data_folder      # data_folder = BCCD_Dataset/BCCD\n",
        "        self.keep_difficult = keep_difficult\n",
        "\n",
        "        # Read data files\n",
        "        with open(os.path.join(data_folder, self.split + '_images.json'), 'r') as j:\n",
        "            self.images = json.load(j)\n",
        "        with open(os.path.join(data_folder, self.split + '_objects.json'), 'r') as j:\n",
        "            self.objects = json.load(j)\n",
        "\n",
        "        assert len(self.images) == len(self.objects)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Read image\n",
        "        image = Image.open(self.images[i], mode='r')\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        # Read objects in this image (bounding boxes, labels, difficulties)\n",
        "        objects = self.objects[i]\n",
        "        boxes = torch.FloatTensor(objects['boxes'])  # (n_objects, 4)\n",
        "        labels = torch.LongTensor(objects['labels'])  # (n_objects)\n",
        "        difficulties = torch.ByteTensor(objects['difficulties'])  # (n_objects)\n",
        "\n",
        "        # Discard difficult objects, if desired\n",
        "        if not self.keep_difficult:\n",
        "            boxes = boxes[1 - difficulties]\n",
        "            labels = labels[1 - difficulties]\n",
        "            difficulties = difficulties[1 - difficulties]\n",
        "\n",
        "        # Apply transformations\n",
        "        image, boxes, labels, difficulties = transform(image, boxes, labels, difficulties, split=self.split) # !!!\n",
        "\n",
        "        return image, boxes, labels, difficulties\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"\n",
        "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
        "\n",
        "        This describes how to combine these tensors of different sizes. We use lists.\n",
        "\n",
        "        Note: this need not be defined in this Class, can be standalone.\n",
        "\n",
        "        :param batch: an iterable of N sets from __getitem__()\n",
        "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
        "        \"\"\"\n",
        "\n",
        "        images = list()\n",
        "        boxes = list()\n",
        "        labels = list()\n",
        "        difficulties = list()\n",
        "\n",
        "        for b in batch:\n",
        "            images.append(b[0])\n",
        "            boxes.append(b[1])\n",
        "            labels.append(b[2])\n",
        "            difficulties.append(b[3])\n",
        "\n",
        "        images = torch.stack(images, dim=0)\n",
        "\n",
        "        return images, boxes, labels, difficulties  # tensor (N, 3, 300, 300), 3 lists of N tensors each"
      ],
      "metadata": {
        "id": "0H-XA1YtmGHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model."
      ],
      "metadata": {
        "id": "jdfq_gKJA7Ja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для изменения тензоров при преобразовании полносвязных слоев в сверточные меньшей размерности."
      ],
      "metadata": {
        "id": "e3LtGtKmc_sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decimate(tensor, m):\n",
        "    \"\"\"\n",
        "    Decimate a tensor by a factor 'm', i.e. downsample by keeping every 'm'th value.\n",
        "\n",
        "    This is used when we convert FC layers to equivalent Convolutional layers, BUT of a smaller size.\n",
        "\n",
        "    :param tensor: tensor to be decimated\n",
        "    :param m: list of decimation factors for each dimension of the tensor; None if not to be decimated along a dimension\n",
        "    :return: decimated tensor\n",
        "    \"\"\"\n",
        "    assert tensor.dim() == len(m)\n",
        "    for d in range(tensor.dim()):\n",
        "        if m[d] is not None:\n",
        "            tensor = tensor.index_select(dim=d,\n",
        "                                         index=torch.arange(start=0, end=tensor.size(d), step=m[d]).long())\n",
        "\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "XsrMq4k1SuXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции для преобразования координат рамок, обрамляющих распознанные объекты (bounding box)."
      ],
      "metadata": {
        "id": "dzcCqU9zdVKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xy_to_cxcy(xy):\n",
        "    \"\"\"\n",
        "    Convert bounding boxes from boundary coordinates (x_min, y_min, x_max, y_max) to center-size coordinates (c_x, c_y, w, h).\n",
        "\n",
        "    :param xy: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
        "    :return: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
        "    \"\"\"\n",
        "    return torch.cat([(xy[:, 2:] + xy[:, :2]) / 2,  # c_x, c_y\n",
        "                      xy[:, 2:] - xy[:, :2]], 1)  # w, h\n",
        "\n",
        "\n",
        "def cxcy_to_xy(cxcy):\n",
        "    \"\"\"\n",
        "    Convert bounding boxes from center-size coordinates (c_x, c_y, w, h) to boundary coordinates (x_min, y_min, x_max, y_max).\n",
        "\n",
        "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_boxes, 4)\n",
        "    :return: bounding boxes in boundary coordinates, a tensor of size (n_boxes, 4)\n",
        "    \"\"\"\n",
        "    return torch.cat([cxcy[:, :2] - (cxcy[:, 2:] / 2),  # x_min, y_min\n",
        "                      cxcy[:, :2] + (cxcy[:, 2:] / 2)], 1)  # x_max, y_max\n",
        "\n",
        "\n",
        "def cxcy_to_gcxgcy(cxcy, priors_cxcy):\n",
        "    \"\"\"\n",
        "    Encode bounding boxes (that are in center-size form) w.r.t. the corresponding prior boxes (that are in center-size form).\n",
        "\n",
        "    For the center coordinates, find the offset with respect to the prior box, and scale by the size of the prior box.\n",
        "    For the size coordinates, scale by the size of the prior box, and convert to the log-space.\n",
        "\n",
        "    In the model, we are predicting bounding box coordinates in this encoded form.\n",
        "\n",
        "    :param cxcy: bounding boxes in center-size coordinates, a tensor of size (n_priors, 4)\n",
        "    :param priors_cxcy: prior boxes with respect to which the encoding must be performed, a tensor of size (n_priors, 4)\n",
        "    :return: encoded bounding boxes, a tensor of size (n_priors, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    # The 10 and 5 below are referred to as 'variances' in the original Caffe repo, completely empirical\n",
        "    # They are for some sort of numerical conditioning, for 'scaling the localization gradient'\n",
        "    # See https://github.com/weiliu89/caffe/issues/155\n",
        "    return torch.cat([(cxcy[:, :2] - priors_cxcy[:, :2]) / (priors_cxcy[:, 2:] / 10),  # g_c_x, g_c_y\n",
        "                      torch.log(cxcy[:, 2:] / priors_cxcy[:, 2:]) * 5], 1)  # g_w, g_h\n",
        "\n",
        "\n",
        "def gcxgcy_to_cxcy(gcxgcy, priors_cxcy):\n",
        "    \"\"\"\n",
        "    Decode bounding box coordinates predicted by the model, since they are encoded in the form mentioned above.\n",
        "\n",
        "    They are decoded into center-size coordinates.\n",
        "\n",
        "    This is the inverse of the function above.\n",
        "\n",
        "    :param gcxgcy: encoded bounding boxes, i.e. output of the model, a tensor of size (n_priors, 4)\n",
        "    :param priors_cxcy: prior boxes with respect to which the encoding is defined, a tensor of size (n_priors, 4)\n",
        "    :return: decoded bounding boxes in center-size form, a tensor of size (n_priors, 4)\n",
        "    \"\"\"\n",
        "\n",
        "    return torch.cat([gcxgcy[:, :2] * priors_cxcy[:, 2:] / 10 + priors_cxcy[:, :2],  # c_x, c_y\n",
        "                      torch.exp(gcxgcy[:, 2:] / 5) * priors_cxcy[:, 2:]], 1)  # w, h"
      ],
      "metadata": {
        "id": "bmYltqz1Heui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс модели на основе сверточной сети VGG, класс лосс-функции MultiBox."
      ],
      "metadata": {
        "id": "bWjFoEL6dvbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from math import sqrt\n",
        "from itertools import product as product\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class VGGBase(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG base convolutions to produce lower-level feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(VGGBase, self).__init__()\n",
        "\n",
        "        # Standard convolutional layers in VGG16\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)  # stride = 1, by default\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)  # ceiling (not floor) here for even dims\n",
        "\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)  # retains size because stride is 1 (and padding)\n",
        "\n",
        "        # Replacements for FC6 and FC7 in VGG16\n",
        "        self.conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)  # atrous convolution\n",
        "\n",
        "        self.conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "\n",
        "        # Load pretrained layers\n",
        "        self.load_pretrained_layers()\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
        "        :return: lower-level feature maps conv4_3 and conv7\n",
        "        \"\"\"\n",
        "        out = F.relu(self.conv1_1(image))  # (N, 64, 300, 300)\n",
        "        out = F.relu(self.conv1_2(out))  # (N, 64, 300, 300)\n",
        "        out = self.pool1(out)  # (N, 64, 150, 150)\n",
        "\n",
        "        out = F.relu(self.conv2_1(out))  # (N, 128, 150, 150)\n",
        "        out = F.relu(self.conv2_2(out))  # (N, 128, 150, 150)\n",
        "        out = self.pool2(out)  # (N, 128, 75, 75)\n",
        "\n",
        "        out = F.relu(self.conv3_1(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_2(out))  # (N, 256, 75, 75)\n",
        "        out = F.relu(self.conv3_3(out))  # (N, 256, 75, 75)\n",
        "        out = self.pool3(out)  # (N, 256, 38, 38), it would have been 37 if not for ceil_mode = True\n",
        "\n",
        "        out = F.relu(self.conv4_1(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_2(out))  # (N, 512, 38, 38)\n",
        "        out = F.relu(self.conv4_3(out))  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = out  # (N, 512, 38, 38)\n",
        "        out = self.pool4(out)  # (N, 512, 19, 19)\n",
        "\n",
        "        out = F.relu(self.conv5_1(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_2(out))  # (N, 512, 19, 19)\n",
        "        out = F.relu(self.conv5_3(out))  # (N, 512, 19, 19)\n",
        "        out = self.pool5(out)  # (N, 512, 19, 19), pool5 does not reduce dimensions\n",
        "\n",
        "        out = F.relu(self.conv6(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        conv7_feats = F.relu(self.conv7(out))  # (N, 1024, 19, 19)\n",
        "\n",
        "        # Lower-level feature maps\n",
        "        return conv4_3_feats, conv7_feats\n",
        "\n",
        "    def load_pretrained_layers(self):\n",
        "        \"\"\"\n",
        "        As in the paper, we use a VGG-16 pretrained on the ImageNet task as the base network.\n",
        "        There's one available in PyTorch, see https://pytorch.org/docs/stable/torchvision/models.html#torchvision.models.vgg16\n",
        "        We copy these parameters into our network. It's straightforward for conv1 to conv5.\n",
        "        However, the original VGG-16 does not contain the conv6 and con7 layers.\n",
        "        Therefore, we convert fc6 and fc7 into convolutional layers, and subsample by decimation. See 'decimate' in utils.py.\n",
        "        \"\"\"\n",
        "        # Current state of base\n",
        "        state_dict = self.state_dict()\n",
        "        param_names = list(state_dict.keys())\n",
        "\n",
        "        # Pretrained VGG base\n",
        "        pretrained_state_dict = torchvision.models.vgg16(pretrained=True).state_dict()\n",
        "        pretrained_param_names = list(pretrained_state_dict.keys())\n",
        "\n",
        "        # Transfer conv. parameters from pretrained model to current model\n",
        "        for i, param in enumerate(param_names[:-4]):  # excluding conv6 and conv7 parameters\n",
        "            state_dict[param] = pretrained_state_dict[pretrained_param_names[i]]\n",
        "\n",
        "        # Convert fc6, fc7 to convolutional layers, and subsample (by decimation) to sizes of conv6 and conv7\n",
        "        # fc6\n",
        "        conv_fc6_weight = pretrained_state_dict['classifier.0.weight'].view(4096, 512, 7, 7)  # (4096, 512, 7, 7)\n",
        "        conv_fc6_bias = pretrained_state_dict['classifier.0.bias']  # (4096)\n",
        "        state_dict['conv6.weight'] = decimate(conv_fc6_weight, m=[4, None, 3, 3])  # (1024, 512, 3, 3)\n",
        "        state_dict['conv6.bias'] = decimate(conv_fc6_bias, m=[4])  # (1024)\n",
        "        # fc7\n",
        "        conv_fc7_weight = pretrained_state_dict['classifier.3.weight'].view(4096, 4096, 1, 1)  # (4096, 4096, 1, 1)\n",
        "        conv_fc7_bias = pretrained_state_dict['classifier.3.bias']  # (4096)\n",
        "        state_dict['conv7.weight'] = decimate(conv_fc7_weight, m=[4, 4, None, None])  # (1024, 1024, 1, 1)\n",
        "        state_dict['conv7.bias'] = decimate(conv_fc7_bias, m=[4])  # (1024)\n",
        "\n",
        "        # Note: an FC layer of size (K) operating on a flattened version (C*H*W) of a 2D image of size (C, H, W)...\n",
        "        # ...is equivalent to a convolutional layer with kernel size (H, W), input channels C, output channels K...\n",
        "        # ...operating on the 2D image of size (C, H, W) without padding\n",
        "\n",
        "        self.load_state_dict(state_dict)\n",
        "\n",
        "        print(\"\\nLoaded base model.\\n\")\n",
        "\n",
        "\n",
        "class AuxiliaryConvolutions(nn.Module):\n",
        "    \"\"\"\n",
        "    Additional convolutions to produce higher-level feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(AuxiliaryConvolutions, self).__init__()\n",
        "\n",
        "        # Auxiliary/additional convolutions on top of the VGG base\n",
        "        self.conv8_1 = nn.Conv2d(1024, 256, kernel_size=1, padding=0)  # stride = 1, by default\n",
        "        self.conv8_2 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
        "\n",
        "        self.conv9_1 = nn.Conv2d(512, 128, kernel_size=1, padding=0)\n",
        "        self.conv9_2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)  # dim. reduction because stride > 1\n",
        "\n",
        "        self.conv10_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
        "        self.conv10_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
        "\n",
        "        self.conv11_1 = nn.Conv2d(256, 128, kernel_size=1, padding=0)\n",
        "        self.conv11_2 = nn.Conv2d(128, 256, kernel_size=3, padding=0)  # dim. reduction because padding = 0\n",
        "\n",
        "        # Initialize convolutions' parameters\n",
        "        self.init_conv2d()\n",
        "\n",
        "    def init_conv2d(self):\n",
        "        \"\"\"\n",
        "        Initialize convolution parameters.\n",
        "        \"\"\"\n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "\n",
        "    def forward(self, conv7_feats):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param conv7_feats: lower-level conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
        "        :return: higher-level feature maps conv8_2, conv9_2, conv10_2, and conv11_2\n",
        "        \"\"\"\n",
        "        out = F.relu(self.conv8_1(conv7_feats))  # (N, 256, 19, 19)\n",
        "        out = F.relu(self.conv8_2(out))  # (N, 512, 10, 10)\n",
        "        conv8_2_feats = out  # (N, 512, 10, 10)\n",
        "\n",
        "        out = F.relu(self.conv9_1(out))  # (N, 128, 10, 10)\n",
        "        out = F.relu(self.conv9_2(out))  # (N, 256, 5, 5)\n",
        "        conv9_2_feats = out  # (N, 256, 5, 5)\n",
        "\n",
        "        out = F.relu(self.conv10_1(out))  # (N, 128, 5, 5)\n",
        "        out = F.relu(self.conv10_2(out))  # (N, 256, 3, 3)\n",
        "        conv10_2_feats = out  # (N, 256, 3, 3)\n",
        "\n",
        "        out = F.relu(self.conv11_1(out))  # (N, 128, 3, 3)\n",
        "        conv11_2_feats = F.relu(self.conv11_2(out))  # (N, 256, 1, 1)\n",
        "\n",
        "        # Higher-level feature maps\n",
        "        return conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats\n",
        "\n",
        "\n",
        "class PredictionConvolutions(nn.Module):\n",
        "    \"\"\"\n",
        "    Convolutions to predict class scores and bounding boxes using lower and higher-level feature maps.\n",
        "\n",
        "    The bounding boxes (locations) are predicted as encoded offsets w.r.t each of the 8732 prior (default) boxes.\n",
        "    See 'cxcy_to_gcxgcy' in utils.py for the encoding definition.\n",
        "\n",
        "    The class scores represent the scores of each object class in each of the 8732 bounding boxes located.\n",
        "    A high score for 'background' = no object.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        \"\"\"\n",
        "        :param n_classes: number of different types of objects\n",
        "        \"\"\"\n",
        "        super(PredictionConvolutions, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Number of prior-boxes we are considering per position in each feature map\n",
        "        n_boxes = {'conv4_3': 4,\n",
        "                   'conv7': 6,\n",
        "                   'conv8_2': 6,\n",
        "                   'conv9_2': 6,\n",
        "                   'conv10_2': 4,\n",
        "                   'conv11_2': 4}\n",
        "        # 4 prior-boxes implies we use 4 different aspect ratios, etc.\n",
        "\n",
        "        # Localization prediction convolutions (predict offsets w.r.t prior-boxes)\n",
        "        self.loc_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * 4, kernel_size=3, padding=1)\n",
        "        self.loc_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * 4, kernel_size=3, padding=1)\n",
        "\n",
        "        # Class prediction convolutions (predict classes in localization boxes)\n",
        "        self.cl_conv4_3 = nn.Conv2d(512, n_boxes['conv4_3'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv7 = nn.Conv2d(1024, n_boxes['conv7'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv8_2 = nn.Conv2d(512, n_boxes['conv8_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv9_2 = nn.Conv2d(256, n_boxes['conv9_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv10_2 = nn.Conv2d(256, n_boxes['conv10_2'] * n_classes, kernel_size=3, padding=1)\n",
        "        self.cl_conv11_2 = nn.Conv2d(256, n_boxes['conv11_2'] * n_classes, kernel_size=3, padding=1)\n",
        "\n",
        "        # Initialize convolutions' parameters\n",
        "        self.init_conv2d()\n",
        "\n",
        "    def init_conv2d(self):\n",
        "        \"\"\"\n",
        "        Initialize convolution parameters.\n",
        "        \"\"\"\n",
        "        for c in self.children():\n",
        "            if isinstance(c, nn.Conv2d):\n",
        "                nn.init.xavier_uniform_(c.weight)\n",
        "                nn.init.constant_(c.bias, 0.)\n",
        "\n",
        "    def forward(self, conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param conv4_3_feats: conv4_3 feature map, a tensor of dimensions (N, 512, 38, 38)\n",
        "        :param conv7_feats: conv7 feature map, a tensor of dimensions (N, 1024, 19, 19)\n",
        "        :param conv8_2_feats: conv8_2 feature map, a tensor of dimensions (N, 512, 10, 10)\n",
        "        :param conv9_2_feats: conv9_2 feature map, a tensor of dimensions (N, 256, 5, 5)\n",
        "        :param conv10_2_feats: conv10_2 feature map, a tensor of dimensions (N, 256, 3, 3)\n",
        "        :param conv11_2_feats: conv11_2 feature map, a tensor of dimensions (N, 256, 1, 1)\n",
        "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
        "        \"\"\"\n",
        "        batch_size = conv4_3_feats.size(0)\n",
        "\n",
        "        # Predict localization boxes' bounds (as offsets w.r.t prior-boxes)\n",
        "        l_conv4_3 = self.loc_conv4_3(conv4_3_feats)  # (N, 16, 38, 38)\n",
        "        l_conv4_3 = l_conv4_3.permute(0, 2, 3,\n",
        "                                      1).contiguous()  # (N, 38, 38, 16), to match prior-box order (after .view())\n",
        "        # (.contiguous() ensures it is stored in a contiguous chunk of memory, needed for .view() below)\n",
        "        l_conv4_3 = l_conv4_3.view(batch_size, -1, 4)  # (N, 5776, 4), there are a total 5776 boxes on this feature map\n",
        "\n",
        "        l_conv7 = self.loc_conv7(conv7_feats)  # (N, 24, 19, 19)\n",
        "        l_conv7 = l_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 24)\n",
        "        l_conv7 = l_conv7.view(batch_size, -1, 4)  # (N, 2166, 4), there are a total 2116 boxes on this feature map\n",
        "\n",
        "        l_conv8_2 = self.loc_conv8_2(conv8_2_feats)  # (N, 24, 10, 10)\n",
        "        l_conv8_2 = l_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 24)\n",
        "        l_conv8_2 = l_conv8_2.view(batch_size, -1, 4)  # (N, 600, 4)\n",
        "\n",
        "        l_conv9_2 = self.loc_conv9_2(conv9_2_feats)  # (N, 24, 5, 5)\n",
        "        l_conv9_2 = l_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 24)\n",
        "        l_conv9_2 = l_conv9_2.view(batch_size, -1, 4)  # (N, 150, 4)\n",
        "\n",
        "        l_conv10_2 = self.loc_conv10_2(conv10_2_feats)  # (N, 16, 3, 3)\n",
        "        l_conv10_2 = l_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 16)\n",
        "        l_conv10_2 = l_conv10_2.view(batch_size, -1, 4)  # (N, 36, 4)\n",
        "\n",
        "        l_conv11_2 = self.loc_conv11_2(conv11_2_feats)  # (N, 16, 1, 1)\n",
        "        l_conv11_2 = l_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 16)\n",
        "        l_conv11_2 = l_conv11_2.view(batch_size, -1, 4)  # (N, 4, 4)\n",
        "\n",
        "        # Predict classes in localization boxes\n",
        "        c_conv4_3 = self.cl_conv4_3(conv4_3_feats)  # (N, 4 * n_classes, 38, 38)\n",
        "        c_conv4_3 = c_conv4_3.permute(0, 2, 3,\n",
        "                                      1).contiguous()  # (N, 38, 38, 4 * n_classes), to match prior-box order (after .view())\n",
        "        c_conv4_3 = c_conv4_3.view(batch_size, -1,\n",
        "                                   self.n_classes)  # (N, 5776, n_classes), there are a total 5776 boxes on this feature map\n",
        "\n",
        "        c_conv7 = self.cl_conv7(conv7_feats)  # (N, 6 * n_classes, 19, 19)\n",
        "        c_conv7 = c_conv7.permute(0, 2, 3, 1).contiguous()  # (N, 19, 19, 6 * n_classes)\n",
        "        c_conv7 = c_conv7.view(batch_size, -1,\n",
        "                               self.n_classes)  # (N, 2166, n_classes), there are a total 2116 boxes on this feature map\n",
        "\n",
        "        c_conv8_2 = self.cl_conv8_2(conv8_2_feats)  # (N, 6 * n_classes, 10, 10)\n",
        "        c_conv8_2 = c_conv8_2.permute(0, 2, 3, 1).contiguous()  # (N, 10, 10, 6 * n_classes)\n",
        "        c_conv8_2 = c_conv8_2.view(batch_size, -1, self.n_classes)  # (N, 600, n_classes)\n",
        "\n",
        "        c_conv9_2 = self.cl_conv9_2(conv9_2_feats)  # (N, 6 * n_classes, 5, 5)\n",
        "        c_conv9_2 = c_conv9_2.permute(0, 2, 3, 1).contiguous()  # (N, 5, 5, 6 * n_classes)\n",
        "        c_conv9_2 = c_conv9_2.view(batch_size, -1, self.n_classes)  # (N, 150, n_classes)\n",
        "\n",
        "        c_conv10_2 = self.cl_conv10_2(conv10_2_feats)  # (N, 4 * n_classes, 3, 3)\n",
        "        c_conv10_2 = c_conv10_2.permute(0, 2, 3, 1).contiguous()  # (N, 3, 3, 4 * n_classes)\n",
        "        c_conv10_2 = c_conv10_2.view(batch_size, -1, self.n_classes)  # (N, 36, n_classes)\n",
        "\n",
        "        c_conv11_2 = self.cl_conv11_2(conv11_2_feats)  # (N, 4 * n_classes, 1, 1)\n",
        "        c_conv11_2 = c_conv11_2.permute(0, 2, 3, 1).contiguous()  # (N, 1, 1, 4 * n_classes)\n",
        "        c_conv11_2 = c_conv11_2.view(batch_size, -1, self.n_classes)  # (N, 4, n_classes)\n",
        "\n",
        "        # A total of 8732 boxes\n",
        "        # Concatenate in this specific order (i.e. must match the order of the prior-boxes)\n",
        "        locs = torch.cat([l_conv4_3, l_conv7, l_conv8_2, l_conv9_2, l_conv10_2, l_conv11_2], dim=1)  # (N, 8732, 4)\n",
        "        classes_scores = torch.cat([c_conv4_3, c_conv7, c_conv8_2, c_conv9_2, c_conv10_2, c_conv11_2],\n",
        "                                   dim=1)  # (N, 8732, n_classes)\n",
        "\n",
        "        return locs, classes_scores\n",
        "\n",
        "\n",
        "class SSD300(nn.Module):\n",
        "    \"\"\"\n",
        "    The SSD300 network - encapsulates the base VGG network, auxiliary, and prediction convolutions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_classes):\n",
        "        super(SSD300, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.base = VGGBase()\n",
        "        self.aux_convs = AuxiliaryConvolutions()\n",
        "        self.pred_convs = PredictionConvolutions(n_classes)\n",
        "\n",
        "        # Since lower level features (conv4_3_feats) have considerably larger scales, we take the L2 norm and rescale\n",
        "        # Rescale factor is initially set at 20, but is learned for each channel during back-prop\n",
        "        self.rescale_factors = nn.Parameter(torch.FloatTensor(1, 512, 1, 1))  # there are 512 channels in conv4_3_feats\n",
        "        nn.init.constant_(self.rescale_factors, 20)\n",
        "\n",
        "        # Prior boxes\n",
        "        self.priors_cxcy = self.create_prior_boxes()\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param image: images, a tensor of dimensions (N, 3, 300, 300)\n",
        "        :return: 8732 locations and class scores (i.e. w.r.t each prior box) for each image\n",
        "        \"\"\"\n",
        "        # Run VGG base network convolutions (lower level feature map generators)\n",
        "        conv4_3_feats, conv7_feats = self.base(image)  # (N, 512, 38, 38), (N, 1024, 19, 19)\n",
        "\n",
        "        # Rescale conv4_3 after L2 norm\n",
        "        norm = conv4_3_feats.pow(2).sum(dim=1, keepdim=True).sqrt()  # (N, 1, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats / norm  # (N, 512, 38, 38)\n",
        "        conv4_3_feats = conv4_3_feats * self.rescale_factors  # (N, 512, 38, 38)\n",
        "        # (PyTorch autobroadcasts singleton dimensions during arithmetic)\n",
        "\n",
        "        # Run auxiliary convolutions (higher level feature map generators)\n",
        "        conv8_2_feats, conv9_2_feats, conv10_2_feats, conv11_2_feats = \\\n",
        "            self.aux_convs(conv7_feats)  # (N, 512, 10, 10),  (N, 256, 5, 5), (N, 256, 3, 3), (N, 256, 1, 1)\n",
        "\n",
        "        # Run prediction convolutions (predict offsets w.r.t prior-boxes and classes in each resulting localization box)\n",
        "        locs, classes_scores = self.pred_convs(conv4_3_feats, conv7_feats, conv8_2_feats, conv9_2_feats, conv10_2_feats,\n",
        "                                               conv11_2_feats)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        return locs, classes_scores\n",
        "\n",
        "    def create_prior_boxes(self):\n",
        "        \"\"\"\n",
        "        Create the 8732 prior (default) boxes for the SSD300, as defined in the paper.\n",
        "\n",
        "        :return: prior boxes in center-size coordinates, a tensor of dimensions (8732, 4)\n",
        "        \"\"\"\n",
        "        fmap_dims = {'conv4_3': 38,\n",
        "                     'conv7': 19,\n",
        "                     'conv8_2': 10,\n",
        "                     'conv9_2': 5,\n",
        "                     'conv10_2': 3,\n",
        "                     'conv11_2': 1}\n",
        "\n",
        "        obj_scales = {'conv4_3': 0.1,\n",
        "                      'conv7': 0.2,\n",
        "                      'conv8_2': 0.375,\n",
        "                      'conv9_2': 0.55,\n",
        "                      'conv10_2': 0.725,\n",
        "                      'conv11_2': 0.9}\n",
        "\n",
        "        aspect_ratios = {'conv4_3': [1., 2., 0.5],\n",
        "                         'conv7': [1., 2., 3., 0.5, .333],\n",
        "                         'conv8_2': [1., 2., 3., 0.5, .333],\n",
        "                         'conv9_2': [1., 2., 3., 0.5, .333],\n",
        "                         'conv10_2': [1., 2., 0.5],\n",
        "                         'conv11_2': [1., 2., 0.5]}\n",
        "\n",
        "        fmaps = list(fmap_dims.keys())\n",
        "\n",
        "        prior_boxes = []\n",
        "\n",
        "        for k, fmap in enumerate(fmaps):\n",
        "            for i in range(fmap_dims[fmap]):\n",
        "                for j in range(fmap_dims[fmap]):\n",
        "                    cx = (j + 0.5) / fmap_dims[fmap]\n",
        "                    cy = (i + 0.5) / fmap_dims[fmap]\n",
        "\n",
        "                    for ratio in aspect_ratios[fmap]:\n",
        "                        prior_boxes.append([cx, cy, obj_scales[fmap] * sqrt(ratio), obj_scales[fmap] / sqrt(ratio)])\n",
        "\n",
        "                        # For an aspect ratio of 1, use an additional prior whose scale is the geometric mean of the\n",
        "                        # scale of the current feature map and the scale of the next feature map\n",
        "                        if ratio == 1.:\n",
        "                            try:\n",
        "                                additional_scale = sqrt(obj_scales[fmap] * obj_scales[fmaps[k + 1]])\n",
        "                            # For the last feature map, there is no \"next\" feature map\n",
        "                            except IndexError:\n",
        "                                additional_scale = 1.\n",
        "                            prior_boxes.append([cx, cy, additional_scale, additional_scale])\n",
        "\n",
        "        prior_boxes = torch.FloatTensor(prior_boxes).to(device)  # (8732, 4)\n",
        "        prior_boxes.clamp_(0, 1)  # (8732, 4); this line has no effect; see Remarks section in tutorial\n",
        "\n",
        "        return prior_boxes\n",
        "\n",
        "    def detect_objects(self, predicted_locs, predicted_scores, min_score, max_overlap, top_k):\n",
        "        \"\"\"\n",
        "        Decipher the 8732 locations and class scores (output of ths SSD300) to detect objects.\n",
        "\n",
        "        For each class, perform Non-Maximum Suppression (NMS) on boxes that are above a minimum threshold.\n",
        "\n",
        "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
        "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
        "        :param min_score: minimum threshold for a box to be considered a match for a certain class\n",
        "        :param max_overlap: maximum overlap two boxes can have so that the one with the lower score is not suppressed via NMS\n",
        "        :param top_k: if there are a lot of resulting detection across all classes, keep only the top 'k'\n",
        "        :return: detections (boxes, labels, and scores), lists of length batch_size\n",
        "        \"\"\"\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "        predicted_scores = F.softmax(predicted_scores, dim=2)  # (N, 8732, n_classes)\n",
        "\n",
        "        # Lists to store final predicted boxes, labels, and scores for all images\n",
        "        all_images_boxes = list()\n",
        "        all_images_labels = list()\n",
        "        all_images_scores = list()\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            # Decode object coordinates from the form we regressed predicted boxes to\n",
        "            decoded_locs = cxcy_to_xy(\n",
        "                gcxgcy_to_cxcy(predicted_locs[i], self.priors_cxcy))  # (8732, 4), these are fractional pt. coordinates\n",
        "\n",
        "            # Lists to store boxes and scores for this image\n",
        "            image_boxes = list()\n",
        "            image_labels = list()\n",
        "            image_scores = list()\n",
        "\n",
        "            max_scores, best_label = predicted_scores[i].max(dim=1)  # (8732)\n",
        "\n",
        "            # Check for each class\n",
        "            for c in range(1, self.n_classes):\n",
        "                # Keep only predicted boxes and scores where scores for this class are above the minimum score\n",
        "                class_scores = predicted_scores[i][:, c]  # (8732)\n",
        "                score_above_min_score = class_scores > min_score  # torch.uint8 (byte) tensor, for indexing\n",
        "                n_above_min_score = score_above_min_score.sum().item()\n",
        "                if n_above_min_score == 0:\n",
        "                    continue\n",
        "                class_scores = class_scores[score_above_min_score]  # (n_qualified), n_min_score <= 8732\n",
        "                class_decoded_locs = decoded_locs[score_above_min_score]  # (n_qualified, 4)\n",
        "\n",
        "                # Sort predicted boxes and scores by scores\n",
        "                class_scores, sort_ind = class_scores.sort(dim=0, descending=True)  # (n_qualified), (n_min_score)\n",
        "                class_decoded_locs = class_decoded_locs[sort_ind]  # (n_min_score, 4)\n",
        "\n",
        "                # Find the overlap between predicted boxes\n",
        "                overlap = find_jaccard_overlap(class_decoded_locs, class_decoded_locs)  # (n_qualified, n_min_score)\n",
        "\n",
        "                # Non-Maximum Suppression (NMS)\n",
        "\n",
        "                # A torch.uint8 (byte) tensor to keep track of which predicted boxes to suppress\n",
        "                # 1 implies suppress, 0 implies don't suppress\n",
        "                suppress = torch.zeros((n_above_min_score), dtype=torch.uint8).to(device)  # (n_qualified)\n",
        "\n",
        "                # Consider each box in order of decreasing scores\n",
        "                for box in range(class_decoded_locs.size(0)):\n",
        "                    # If this box is already marked for suppression\n",
        "                    if suppress[box] == 1:\n",
        "                        continue\n",
        "\n",
        "                    # Suppress boxes whose overlaps (with this box) are greater than maximum overlap\n",
        "                    # Find such boxes and update suppress indices\n",
        "                    suppress = torch.max(suppress, overlap[box] > max_overlap)\n",
        "                    # The max operation retains previously suppressed boxes, like an 'OR' operation\n",
        "\n",
        "                    # Don't suppress this box, even though it has an overlap of 1 with itself\n",
        "                    suppress[box] = 0\n",
        "\n",
        "                # Store only unsuppressed boxes for this class\n",
        "                image_boxes.append(class_decoded_locs[1 - suppress])\n",
        "                image_labels.append(torch.LongTensor((1 - suppress).sum().item() * [c]).to(device))\n",
        "                image_scores.append(class_scores[1 - suppress])\n",
        "\n",
        "            # If no object in any class is found, store a placeholder for 'background'\n",
        "            if len(image_boxes) == 0:\n",
        "                image_boxes.append(torch.FloatTensor([[0., 0., 1., 1.]]).to(device))\n",
        "                image_labels.append(torch.LongTensor([0]).to(device))\n",
        "                image_scores.append(torch.FloatTensor([0.]).to(device))\n",
        "\n",
        "            # Concatenate into single tensors\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)  # (n_objects, 4)\n",
        "            image_labels = torch.cat(image_labels, dim=0)  # (n_objects)\n",
        "            image_scores = torch.cat(image_scores, dim=0)  # (n_objects)\n",
        "            n_objects = image_scores.size(0)\n",
        "\n",
        "            # Keep only the top k objects\n",
        "            if n_objects > top_k:\n",
        "                image_scores, sort_ind = image_scores.sort(dim=0, descending=True)\n",
        "                image_scores = image_scores[:top_k]  # (top_k)\n",
        "                image_boxes = image_boxes[sort_ind][:top_k]  # (top_k, 4)\n",
        "                image_labels = image_labels[sort_ind][:top_k]  # (top_k)\n",
        "\n",
        "            # Append to lists that store predicted boxes and scores for all images\n",
        "            all_images_boxes.append(image_boxes)\n",
        "            all_images_labels.append(image_labels)\n",
        "            all_images_scores.append(image_scores)\n",
        "\n",
        "        return all_images_boxes, all_images_labels, all_images_scores  # lists of length batch_size\n",
        "\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    The MultiBox loss, a loss function for object detection.\n",
        "\n",
        "    This is a combination of:\n",
        "    (1) a localization loss for the predicted locations of the boxes, and\n",
        "    (2) a confidence loss for the predicted class scores.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, priors_cxcy, threshold=0.5, neg_pos_ratio=3, alpha=1.):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.priors_cxcy = priors_cxcy\n",
        "        self.priors_xy = cxcy_to_xy(priors_cxcy)\n",
        "        self.threshold = threshold\n",
        "        self.neg_pos_ratio = neg_pos_ratio\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.smooth_l1 = nn.L1Loss()  # *smooth* L1 loss in the paper; see Remarks section in the tutorial\n",
        "        self.cross_entropy = nn.CrossEntropyLoss(reduce=False)\n",
        "\n",
        "    def forward(self, predicted_locs, predicted_scores, boxes, labels):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param predicted_locs: predicted locations/boxes w.r.t the 8732 prior boxes, a tensor of dimensions (N, 8732, 4)\n",
        "        :param predicted_scores: class scores for each of the encoded locations/boxes, a tensor of dimensions (N, 8732, n_classes)\n",
        "        :param boxes: true  object bounding boxes in boundary coordinates, a list of N tensors\n",
        "        :param labels: true object labels, a list of N tensors\n",
        "        :return: multibox loss, a scalar\n",
        "        \"\"\"\n",
        "        batch_size = predicted_locs.size(0)\n",
        "        n_priors = self.priors_cxcy.size(0)\n",
        "        n_classes = predicted_scores.size(2)\n",
        "\n",
        "        assert n_priors == predicted_locs.size(1) == predicted_scores.size(1)\n",
        "\n",
        "        true_locs = torch.zeros((batch_size, n_priors, 4), dtype=torch.float).to(device)  # (N, 8732, 4)\n",
        "        true_classes = torch.zeros((batch_size, n_priors), dtype=torch.long).to(device)  # (N, 8732)\n",
        "\n",
        "        # For each image\n",
        "        for i in range(batch_size):\n",
        "            n_objects = boxes[i].size(0)\n",
        "\n",
        "            overlap = find_jaccard_overlap(boxes[i],\n",
        "                                           self.priors_xy)  # (n_objects, 8732)\n",
        "\n",
        "            # For each prior, find the object that has the maximum overlap\n",
        "            overlap_for_each_prior, object_for_each_prior = overlap.max(dim=0)  # (8732)\n",
        "\n",
        "            # We don't want a situation where an object is not represented in our positive (non-background) priors -\n",
        "            # 1. An object might not be the best object for all priors, and is therefore not in object_for_each_prior.\n",
        "            # 2. All priors with the object may be assigned as background based on the threshold (0.5).\n",
        "\n",
        "            # To remedy this -\n",
        "            # First, find the prior that has the maximum overlap for each object.\n",
        "            _, prior_for_each_object = overlap.max(dim=1)  # (N_o)\n",
        "\n",
        "            # Then, assign each object to the corresponding maximum-overlap-prior. (This fixes 1.)\n",
        "            object_for_each_prior[prior_for_each_object] = torch.LongTensor(range(n_objects)).to(device)\n",
        "\n",
        "            # To ensure these priors qualify, artificially give them an overlap of greater than 0.5. (This fixes 2.)\n",
        "            overlap_for_each_prior[prior_for_each_object] = 1.\n",
        "\n",
        "            # Labels for each prior\n",
        "            label_for_each_prior = labels[i][object_for_each_prior]  # (8732)\n",
        "            # Set priors whose overlaps with objects are less than the threshold to be background (no object)\n",
        "            label_for_each_prior[overlap_for_each_prior < self.threshold] = 0  # (8732)\n",
        "\n",
        "            # Store\n",
        "            true_classes[i] = label_for_each_prior\n",
        "\n",
        "            # Encode center-size object coordinates into the form we regressed predicted boxes to\n",
        "            true_locs[i] = cxcy_to_gcxgcy(xy_to_cxcy(boxes[i][object_for_each_prior]), self.priors_cxcy)  # (8732, 4)\n",
        "\n",
        "        # Identify priors that are positive (object/non-background)\n",
        "        positive_priors = true_classes != 0  # (N, 8732)\n",
        "\n",
        "        # LOCALIZATION LOSS\n",
        "\n",
        "        # Localization loss is computed only over positive (non-background) priors\n",
        "        loc_loss = self.smooth_l1(predicted_locs[positive_priors], true_locs[positive_priors])  # (), scalar\n",
        "\n",
        "        # Note: indexing with a torch.uint8 (byte) tensor flattens the tensor when indexing is across multiple dimensions (N & 8732)\n",
        "        # So, if predicted_locs has the shape (N, 8732, 4), predicted_locs[positive_priors] will have (total positives, 4)\n",
        "\n",
        "        # CONFIDENCE LOSS\n",
        "\n",
        "        # Confidence loss is computed over positive priors and the most difficult (hardest) negative priors in each image\n",
        "        # That is, FOR EACH IMAGE,\n",
        "        # we will take the hardest (neg_pos_ratio * n_positives) negative priors, i.e where there is maximum loss\n",
        "        # This is called Hard Negative Mining - it concentrates on hardest negatives in each image, and also minimizes pos/neg imbalance\n",
        "\n",
        "        # Number of positive and hard-negative priors per image\n",
        "        n_positives = positive_priors.sum(dim=1)  # (N)\n",
        "        n_hard_negatives = self.neg_pos_ratio * n_positives  # (N)\n",
        "\n",
        "        # First, find the loss for all priors\n",
        "        conf_loss_all = self.cross_entropy(predicted_scores.view(-1, n_classes), true_classes.view(-1))  # (N * 8732)\n",
        "        conf_loss_all = conf_loss_all.view(batch_size, n_priors)  # (N, 8732)\n",
        "\n",
        "        # We already know which priors are positive\n",
        "        conf_loss_pos = conf_loss_all[positive_priors]  # (sum(n_positives))\n",
        "\n",
        "        # Next, find which priors are hard-negative\n",
        "        # To do this, sort ONLY negative priors in each image in order of decreasing loss and take top n_hard_negatives\n",
        "        conf_loss_neg = conf_loss_all.clone()  # (N, 8732)\n",
        "        conf_loss_neg[positive_priors] = 0.  # (N, 8732), positive priors are ignored (never in top n_hard_negatives)\n",
        "        conf_loss_neg, _ = conf_loss_neg.sort(dim=1, descending=True)  # (N, 8732), sorted by decreasing hardness\n",
        "        hardness_ranks = torch.LongTensor(range(n_priors)).unsqueeze(0).expand_as(conf_loss_neg).to(device)  # (N, 8732)\n",
        "        hard_negatives = hardness_ranks < n_hard_negatives.unsqueeze(1)  # (N, 8732)\n",
        "        conf_loss_hard_neg = conf_loss_neg[hard_negatives]  # (sum(n_hard_negatives))\n",
        "\n",
        "        # As in the paper, averaged over positive priors only, although computed over both positive and hard-negative priors\n",
        "        conf_loss = (conf_loss_hard_neg.sum() + conf_loss_pos.sum()) / n_positives.sum().float()  # (), scalar\n",
        "\n",
        "        # TOTAL LOSS\n",
        "\n",
        "        return conf_loss + self.alpha * loc_loss\n"
      ],
      "metadata": {
        "id": "UlQHiY9-Xbi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model."
      ],
      "metadata": {
        "id": "DFFQIAPIIfd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение модели."
      ],
      "metadata": {
        "id": "4nInqsVleUAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для изменения шага обучения."
      ],
      "metadata": {
        "id": "sKIbGt9FecRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, scale):\n",
        "    \"\"\"\n",
        "    Scale learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param scale: factor to multiply learning rate with.\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * scale\n",
        "    print(\"DECAYING learning rate.\\n The new LR is %f\\n\" % (optimizer.param_groups[1]['lr'],))\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer):\n",
        "    \"\"\"\n",
        "    Save model checkpoint.\n",
        "\n",
        "    :param epoch: epoch number\n",
        "    :param model: model\n",
        "    :param optimizer: optimizer\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer}\n",
        "    filename = 'checkpoint_ssd300.pth.tar'\n",
        "    torch.save(state, filename)"
      ],
      "metadata": {
        "id": "jk_e6ecKI0It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Класс для контроля метрики."
      ],
      "metadata": {
        "id": "GjCJm1zZeoSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)"
      ],
      "metadata": {
        "id": "Z3JFeZXjI08x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для обучения изменены следующие параметры:\n",
        "- путь к датасету: data_folder = 'BCCD_Dataset',\n",
        "- параметр workers = 2 для загрузки данных даталоудером,\n",
        "- batch_size уменьшен до 4,\n",
        "- число итераций уменьшено до 2400 (уменьшение шага обучения после 1500 и 2000 итераций), т.о. количество эпох обучения не превысит 266."
      ],
      "metadata": {
        "id": "iK59TBwAe04L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "\n",
        "# Data parameters\n",
        "data_folder = 'BCCD_Dataset'  # folder with data files\n",
        "keep_difficult = True  # use objects considered difficult to detect?\n",
        "\n",
        "# Model parameters\n",
        "# Not too many here since the SSD300 has a very specific structure\n",
        "n_classes = len(label_map)  # number of different types of objects\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Learning parameters\n",
        "checkpoint = None  # path to model checkpoint, None if none\n",
        "batch_size = 4  # batch size\n",
        "iterations = 2400  # number of iterations to train\n",
        "workers = 2  # number of workers for loading data in the DataLoader\n",
        "print_freq = 200  # print training status every __ batches\n",
        "lr = 1e-3  # learning rate\n",
        "decay_lr_at = [1500, 2000]  # decay learning rate after these many iterations\n",
        "decay_lr_to = 0.1  # decay learning rate to this fraction of the existing learning rate\n",
        "momentum = 0.9  # momentum\n",
        "weight_decay = 5e-4  # weight decay\n",
        "grad_clip = None  # clip if gradients are exploding, which may happen at larger batch sizes (sometimes at 32) - you will recognize it by a sorting error in the MuliBox loss calculation\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Training.\n",
        "    \"\"\"\n",
        "    global start_epoch, label_map, epoch, checkpoint, decay_lr_at\n",
        "\n",
        "    # Initialize model or load checkpoint\n",
        "    if checkpoint is None:\n",
        "        start_epoch = 0\n",
        "        model = SSD300(n_classes=n_classes)\n",
        "        # Initialize the optimizer, with twice the default learning rate for biases, as in the original Caffe repo\n",
        "        biases = list()\n",
        "        not_biases = list()\n",
        "        for param_name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if param_name.endswith('.bias'):\n",
        "                    biases.append(param)\n",
        "                else:\n",
        "                    not_biases.append(param)\n",
        "        optimizer = torch.optim.SGD(params=[{'params': biases, 'lr': 2 * lr}, {'params': not_biases}],\n",
        "                                    lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print('\\nLoaded checkpoint from epoch %d.\\n' % start_epoch)\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "\n",
        "    # Move to default device\n",
        "    model = model.to(device)\n",
        "    criterion = MultiBoxLoss(priors_cxcy=model.priors_cxcy).to(device)\n",
        "\n",
        "    # Custom dataloaders\n",
        "    train_dataset = BCCD_Dataset(data_folder,\n",
        "                                     split='train',\n",
        "                                     keep_difficult=keep_difficult)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                                               collate_fn=train_dataset.collate_fn, num_workers=workers,\n",
        "                                               pin_memory=True)  # note that we're passing the collate function here\n",
        "\n",
        "    # Calculate total number of epochs to train and the epochs to decay learning rate at (i.e. convert iterations to epochs)\n",
        "    # To convert iterations to epochs, divide iterations by the number of iterations per epoch\n",
        "    # The paper trains for 120,000 iterations with a batch size of 32, decays after 80,000 and 100,000 iterations\n",
        "    epochs = iterations // (len(train_dataset) // 32)\n",
        "    decay_lr_at = [it // (len(train_dataset) // 32) for it in decay_lr_at]\n",
        "\n",
        "    # Epochs\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        # Decay learning rate at particular epochs\n",
        "        if epoch in decay_lr_at:\n",
        "            adjust_learning_rate(optimizer, decay_lr_to)\n",
        "\n",
        "        # One epoch's training\n",
        "        train(train_loader=train_loader,\n",
        "              model=model,\n",
        "              criterion=criterion,\n",
        "              optimizer=optimizer,\n",
        "              epoch=epoch)\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(epoch, model, optimizer)\n",
        "\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    \"\"\"\n",
        "    One epoch's training.\n",
        "\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param model: model\n",
        "    :param criterion: MultiBox loss\n",
        "    :param optimizer: optimizer\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "    model.train()  # training mode enables dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (images, boxes, labels, _) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to default device\n",
        "        images = images.to(device)  # (batch_size (N), 3, 300, 300)\n",
        "        boxes = [b.to(device) for b in boxes]\n",
        "        labels = [l.to(device) for l in labels]\n",
        "\n",
        "        # Forward prop.\n",
        "        predicted_locs, predicted_scores = model(images)  # (N, 8732, 4), (N, 8732, n_classes)\n",
        "\n",
        "        # Loss\n",
        "        loss = criterion(predicted_locs, predicted_scores, boxes, labels)  # scalar\n",
        "\n",
        "        # Backward prop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients, if necessary\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "\n",
        "        # Update model\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
        "                                                                  batch_time=batch_time,\n",
        "                                                                  data_time=data_time, loss=losses))\n",
        "    del predicted_locs, predicted_scores, images, boxes, labels  # free some memory since their histories may be stored\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "_aO1sWIhIq0A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8eb6ef4-b595-40b1-c73a-55074340a575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded base model.\n",
            "\n",
            "Epoch: [0][0/73]\tBatch Time 0.629 (0.629)\tData Time 0.351 (0.351)\tLoss 15.2448 (15.2448)\t\n",
            "Epoch: [1][0/73]\tBatch Time 0.584 (0.584)\tData Time 0.401 (0.401)\tLoss 6.0605 (6.0605)\t\n",
            "Epoch: [2][0/73]\tBatch Time 0.607 (0.607)\tData Time 0.426 (0.426)\tLoss 3.7566 (3.7566)\t\n",
            "Epoch: [3][0/73]\tBatch Time 0.813 (0.813)\tData Time 0.638 (0.638)\tLoss 3.4328 (3.4328)\t\n",
            "Epoch: [4][0/73]\tBatch Time 0.738 (0.738)\tData Time 0.554 (0.554)\tLoss 3.0476 (3.0476)\t\n",
            "Epoch: [5][0/73]\tBatch Time 0.680 (0.680)\tData Time 0.504 (0.504)\tLoss 3.3417 (3.3417)\t\n",
            "Epoch: [6][0/73]\tBatch Time 0.991 (0.991)\tData Time 0.827 (0.827)\tLoss 3.1309 (3.1309)\t\n",
            "Epoch: [7][0/73]\tBatch Time 0.866 (0.866)\tData Time 0.694 (0.694)\tLoss 3.1417 (3.1417)\t\n",
            "Epoch: [8][0/73]\tBatch Time 0.568 (0.568)\tData Time 0.391 (0.391)\tLoss 2.7546 (2.7546)\t\n",
            "Epoch: [9][0/73]\tBatch Time 1.526 (1.526)\tData Time 1.245 (1.245)\tLoss 2.7771 (2.7771)\t\n",
            "Epoch: [10][0/73]\tBatch Time 1.121 (1.121)\tData Time 0.955 (0.955)\tLoss 3.0131 (3.0131)\t\n",
            "Epoch: [11][0/73]\tBatch Time 0.614 (0.614)\tData Time 0.441 (0.441)\tLoss 2.7380 (2.7380)\t\n",
            "Epoch: [12][0/73]\tBatch Time 1.135 (1.135)\tData Time 0.975 (0.975)\tLoss 2.7183 (2.7183)\t\n",
            "Epoch: [13][0/73]\tBatch Time 0.843 (0.843)\tData Time 0.668 (0.668)\tLoss 3.0410 (3.0410)\t\n",
            "Epoch: [14][0/73]\tBatch Time 0.818 (0.818)\tData Time 0.637 (0.637)\tLoss 2.3869 (2.3869)\t\n",
            "Epoch: [15][0/73]\tBatch Time 0.907 (0.907)\tData Time 0.731 (0.731)\tLoss 2.6696 (2.6696)\t\n",
            "Epoch: [16][0/73]\tBatch Time 0.832 (0.832)\tData Time 0.628 (0.628)\tLoss 2.3767 (2.3767)\t\n",
            "Epoch: [17][0/73]\tBatch Time 0.525 (0.525)\tData Time 0.328 (0.328)\tLoss 2.8300 (2.8300)\t\n",
            "Epoch: [18][0/73]\tBatch Time 1.201 (1.201)\tData Time 1.002 (1.002)\tLoss 2.2955 (2.2955)\t\n",
            "Epoch: [19][0/73]\tBatch Time 0.536 (0.536)\tData Time 0.343 (0.343)\tLoss 3.1560 (3.1560)\t\n",
            "Epoch: [20][0/73]\tBatch Time 0.819 (0.819)\tData Time 0.634 (0.634)\tLoss 2.5469 (2.5469)\t\n",
            "Epoch: [21][0/73]\tBatch Time 0.875 (0.875)\tData Time 0.686 (0.686)\tLoss 2.8461 (2.8461)\t\n",
            "Epoch: [22][0/73]\tBatch Time 1.024 (1.024)\tData Time 0.848 (0.848)\tLoss 2.5124 (2.5124)\t\n",
            "Epoch: [23][0/73]\tBatch Time 0.875 (0.875)\tData Time 0.683 (0.683)\tLoss 2.4906 (2.4906)\t\n",
            "Epoch: [24][0/73]\tBatch Time 0.872 (0.872)\tData Time 0.688 (0.688)\tLoss 2.8469 (2.8469)\t\n",
            "Epoch: [25][0/73]\tBatch Time 1.371 (1.371)\tData Time 1.138 (1.138)\tLoss 2.9170 (2.9170)\t\n",
            "Epoch: [26][0/73]\tBatch Time 0.885 (0.885)\tData Time 0.720 (0.720)\tLoss 2.8476 (2.8476)\t\n",
            "Epoch: [27][0/73]\tBatch Time 0.570 (0.570)\tData Time 0.384 (0.384)\tLoss 2.2888 (2.2888)\t\n",
            "Epoch: [28][0/73]\tBatch Time 0.546 (0.546)\tData Time 0.364 (0.364)\tLoss 2.3972 (2.3972)\t\n",
            "Epoch: [29][0/73]\tBatch Time 0.775 (0.775)\tData Time 0.603 (0.603)\tLoss 2.7876 (2.7876)\t\n",
            "Epoch: [30][0/73]\tBatch Time 0.868 (0.868)\tData Time 0.686 (0.686)\tLoss 2.6536 (2.6536)\t\n",
            "Epoch: [31][0/73]\tBatch Time 0.932 (0.932)\tData Time 0.735 (0.735)\tLoss 2.2671 (2.2671)\t\n",
            "Epoch: [32][0/73]\tBatch Time 0.889 (0.889)\tData Time 0.679 (0.679)\tLoss 2.1605 (2.1605)\t\n",
            "Epoch: [33][0/73]\tBatch Time 0.558 (0.558)\tData Time 0.382 (0.382)\tLoss 2.5259 (2.5259)\t\n",
            "Epoch: [34][0/73]\tBatch Time 0.597 (0.597)\tData Time 0.418 (0.418)\tLoss 2.9371 (2.9371)\t\n",
            "Epoch: [35][0/73]\tBatch Time 0.867 (0.867)\tData Time 0.686 (0.686)\tLoss 2.1886 (2.1886)\t\n",
            "Epoch: [36][0/73]\tBatch Time 0.719 (0.719)\tData Time 0.528 (0.528)\tLoss 2.5663 (2.5663)\t\n",
            "Epoch: [37][0/73]\tBatch Time 0.546 (0.546)\tData Time 0.371 (0.371)\tLoss 2.3104 (2.3104)\t\n",
            "Epoch: [38][0/73]\tBatch Time 0.860 (0.860)\tData Time 0.683 (0.683)\tLoss 2.6796 (2.6796)\t\n",
            "Epoch: [39][0/73]\tBatch Time 0.646 (0.646)\tData Time 0.476 (0.476)\tLoss 2.1691 (2.1691)\t\n",
            "Epoch: [40][0/73]\tBatch Time 0.780 (0.780)\tData Time 0.607 (0.607)\tLoss 2.7007 (2.7007)\t\n",
            "Epoch: [41][0/73]\tBatch Time 0.814 (0.814)\tData Time 0.640 (0.640)\tLoss 2.5479 (2.5479)\t\n",
            "Epoch: [42][0/73]\tBatch Time 0.820 (0.820)\tData Time 0.647 (0.647)\tLoss 2.8261 (2.8261)\t\n",
            "Epoch: [43][0/73]\tBatch Time 0.664 (0.664)\tData Time 0.482 (0.482)\tLoss 2.3446 (2.3446)\t\n",
            "Epoch: [44][0/73]\tBatch Time 0.681 (0.681)\tData Time 0.507 (0.507)\tLoss 2.5414 (2.5414)\t\n",
            "Epoch: [45][0/73]\tBatch Time 0.907 (0.907)\tData Time 0.745 (0.745)\tLoss 2.5448 (2.5448)\t\n",
            "Epoch: [46][0/73]\tBatch Time 0.809 (0.809)\tData Time 0.632 (0.632)\tLoss 2.2268 (2.2268)\t\n",
            "Epoch: [47][0/73]\tBatch Time 0.534 (0.534)\tData Time 0.357 (0.357)\tLoss 2.3534 (2.3534)\t\n",
            "Epoch: [48][0/73]\tBatch Time 0.838 (0.838)\tData Time 0.663 (0.663)\tLoss 2.3247 (2.3247)\t\n",
            "Epoch: [49][0/73]\tBatch Time 0.864 (0.864)\tData Time 0.681 (0.681)\tLoss 3.4354 (3.4354)\t\n",
            "Epoch: [50][0/73]\tBatch Time 1.147 (1.147)\tData Time 0.980 (0.980)\tLoss 2.7729 (2.7729)\t\n",
            "Epoch: [51][0/73]\tBatch Time 0.939 (0.939)\tData Time 0.748 (0.748)\tLoss 2.0659 (2.0659)\t\n",
            "Epoch: [52][0/73]\tBatch Time 0.565 (0.565)\tData Time 0.381 (0.381)\tLoss 2.4466 (2.4466)\t\n",
            "Epoch: [53][0/73]\tBatch Time 1.142 (1.142)\tData Time 0.975 (0.975)\tLoss 2.9236 (2.9236)\t\n",
            "Epoch: [54][0/73]\tBatch Time 0.559 (0.559)\tData Time 0.369 (0.369)\tLoss 2.6280 (2.6280)\t\n",
            "Epoch: [55][0/73]\tBatch Time 0.947 (0.947)\tData Time 0.767 (0.767)\tLoss 2.2799 (2.2799)\t\n",
            "Epoch: [56][0/73]\tBatch Time 1.038 (1.038)\tData Time 0.850 (0.850)\tLoss 2.0977 (2.0977)\t\n",
            "Epoch: [57][0/73]\tBatch Time 0.937 (0.937)\tData Time 0.754 (0.754)\tLoss 2.5945 (2.5945)\t\n",
            "Epoch: [58][0/73]\tBatch Time 0.834 (0.834)\tData Time 0.658 (0.658)\tLoss 2.4948 (2.4948)\t\n",
            "Epoch: [59][0/73]\tBatch Time 0.873 (0.873)\tData Time 0.699 (0.699)\tLoss 2.4906 (2.4906)\t\n",
            "Epoch: [60][0/73]\tBatch Time 1.046 (1.046)\tData Time 0.870 (0.870)\tLoss 2.2844 (2.2844)\t\n",
            "Epoch: [61][0/73]\tBatch Time 1.003 (1.003)\tData Time 0.834 (0.834)\tLoss 2.2489 (2.2489)\t\n",
            "Epoch: [62][0/73]\tBatch Time 0.557 (0.557)\tData Time 0.371 (0.371)\tLoss 2.5856 (2.5856)\t\n",
            "Epoch: [63][0/73]\tBatch Time 0.593 (0.593)\tData Time 0.408 (0.408)\tLoss 2.0918 (2.0918)\t\n",
            "Epoch: [64][0/73]\tBatch Time 0.907 (0.907)\tData Time 0.701 (0.701)\tLoss 2.3217 (2.3217)\t\n",
            "Epoch: [65][0/73]\tBatch Time 0.799 (0.799)\tData Time 0.621 (0.621)\tLoss 2.8190 (2.8190)\t\n",
            "Epoch: [66][0/73]\tBatch Time 0.752 (0.752)\tData Time 0.568 (0.568)\tLoss 2.1717 (2.1717)\t\n",
            "Epoch: [67][0/73]\tBatch Time 0.743 (0.743)\tData Time 0.550 (0.550)\tLoss 2.1942 (2.1942)\t\n",
            "Epoch: [68][0/73]\tBatch Time 0.999 (0.999)\tData Time 0.837 (0.837)\tLoss 2.2916 (2.2916)\t\n",
            "Epoch: [69][0/73]\tBatch Time 0.922 (0.922)\tData Time 0.745 (0.745)\tLoss 2.2556 (2.2556)\t\n",
            "Epoch: [70][0/73]\tBatch Time 0.805 (0.805)\tData Time 0.628 (0.628)\tLoss 1.9831 (1.9831)\t\n",
            "Epoch: [71][0/73]\tBatch Time 0.693 (0.693)\tData Time 0.515 (0.515)\tLoss 2.4096 (2.4096)\t\n",
            "Epoch: [72][0/73]\tBatch Time 0.659 (0.659)\tData Time 0.473 (0.473)\tLoss 2.5955 (2.5955)\t\n",
            "Epoch: [73][0/73]\tBatch Time 0.896 (0.896)\tData Time 0.722 (0.722)\tLoss 1.8227 (1.8227)\t\n",
            "Epoch: [74][0/73]\tBatch Time 0.635 (0.635)\tData Time 0.440 (0.440)\tLoss 1.8646 (1.8646)\t\n",
            "Epoch: [75][0/73]\tBatch Time 0.575 (0.575)\tData Time 0.369 (0.369)\tLoss 2.1347 (2.1347)\t\n",
            "Epoch: [76][0/73]\tBatch Time 0.658 (0.658)\tData Time 0.453 (0.453)\tLoss 2.2722 (2.2722)\t\n",
            "Epoch: [77][0/73]\tBatch Time 1.118 (1.118)\tData Time 0.949 (0.949)\tLoss 2.2768 (2.2768)\t\n",
            "Epoch: [78][0/73]\tBatch Time 0.560 (0.560)\tData Time 0.374 (0.374)\tLoss 2.3223 (2.3223)\t\n",
            "Epoch: [79][0/73]\tBatch Time 1.096 (1.096)\tData Time 0.912 (0.912)\tLoss 2.5394 (2.5394)\t\n",
            "Epoch: [80][0/73]\tBatch Time 0.868 (0.868)\tData Time 0.691 (0.691)\tLoss 2.2453 (2.2453)\t\n",
            "Epoch: [81][0/73]\tBatch Time 1.069 (1.069)\tData Time 0.894 (0.894)\tLoss 2.7227 (2.7227)\t\n",
            "Epoch: [82][0/73]\tBatch Time 0.872 (0.872)\tData Time 0.688 (0.688)\tLoss 2.2019 (2.2019)\t\n",
            "Epoch: [83][0/73]\tBatch Time 0.497 (0.497)\tData Time 0.318 (0.318)\tLoss 2.6012 (2.6012)\t\n",
            "Epoch: [84][0/73]\tBatch Time 0.778 (0.778)\tData Time 0.592 (0.592)\tLoss 2.0450 (2.0450)\t\n",
            "Epoch: [85][0/73]\tBatch Time 0.789 (0.789)\tData Time 0.595 (0.595)\tLoss 1.9372 (1.9372)\t\n",
            "Epoch: [86][0/73]\tBatch Time 0.822 (0.822)\tData Time 0.659 (0.659)\tLoss 2.5064 (2.5064)\t\n",
            "Epoch: [87][0/73]\tBatch Time 0.714 (0.714)\tData Time 0.538 (0.538)\tLoss 2.2313 (2.2313)\t\n",
            "Epoch: [88][0/73]\tBatch Time 0.880 (0.880)\tData Time 0.674 (0.674)\tLoss 2.2793 (2.2793)\t\n",
            "Epoch: [89][0/73]\tBatch Time 0.649 (0.649)\tData Time 0.469 (0.469)\tLoss 2.2787 (2.2787)\t\n",
            "Epoch: [90][0/73]\tBatch Time 0.660 (0.660)\tData Time 0.479 (0.479)\tLoss 2.2219 (2.2219)\t\n",
            "Epoch: [91][0/73]\tBatch Time 0.673 (0.673)\tData Time 0.506 (0.506)\tLoss 2.2519 (2.2519)\t\n",
            "Epoch: [92][0/73]\tBatch Time 0.848 (0.848)\tData Time 0.666 (0.666)\tLoss 1.9193 (1.9193)\t\n",
            "Epoch: [93][0/73]\tBatch Time 0.709 (0.709)\tData Time 0.503 (0.503)\tLoss 2.0205 (2.0205)\t\n",
            "Epoch: [94][0/73]\tBatch Time 0.760 (0.760)\tData Time 0.583 (0.583)\tLoss 2.1377 (2.1377)\t\n",
            "Epoch: [95][0/73]\tBatch Time 0.721 (0.721)\tData Time 0.536 (0.536)\tLoss 1.9922 (1.9922)\t\n",
            "Epoch: [96][0/73]\tBatch Time 0.746 (0.746)\tData Time 0.578 (0.578)\tLoss inf (inf)\t\n",
            "Epoch: [97][0/73]\tBatch Time 0.601 (0.601)\tData Time 0.417 (0.417)\tLoss 2.4142 (2.4142)\t\n",
            "Epoch: [98][0/73]\tBatch Time 1.135 (1.135)\tData Time 0.873 (0.873)\tLoss 1.9062 (1.9062)\t\n",
            "Epoch: [99][0/73]\tBatch Time 0.865 (0.865)\tData Time 0.701 (0.701)\tLoss 2.1367 (2.1367)\t\n",
            "Epoch: [100][0/73]\tBatch Time 0.957 (0.957)\tData Time 0.784 (0.784)\tLoss 1.9091 (1.9091)\t\n",
            "Epoch: [101][0/73]\tBatch Time 0.727 (0.727)\tData Time 0.563 (0.563)\tLoss 2.4344 (2.4344)\t\n",
            "Epoch: [102][0/73]\tBatch Time 0.673 (0.673)\tData Time 0.490 (0.490)\tLoss 2.0160 (2.0160)\t\n",
            "Epoch: [103][0/73]\tBatch Time 0.962 (0.962)\tData Time 0.773 (0.773)\tLoss 1.9306 (1.9306)\t\n",
            "Epoch: [104][0/73]\tBatch Time 0.883 (0.883)\tData Time 0.701 (0.701)\tLoss 2.1860 (2.1860)\t\n",
            "Epoch: [105][0/73]\tBatch Time 0.746 (0.746)\tData Time 0.581 (0.581)\tLoss 2.4007 (2.4007)\t\n",
            "Epoch: [106][0/73]\tBatch Time 0.634 (0.634)\tData Time 0.456 (0.456)\tLoss 2.4486 (2.4486)\t\n",
            "Epoch: [107][0/73]\tBatch Time 0.878 (0.878)\tData Time 0.709 (0.709)\tLoss 2.4420 (2.4420)\t\n",
            "Epoch: [108][0/73]\tBatch Time 0.675 (0.675)\tData Time 0.501 (0.501)\tLoss 2.2481 (2.2481)\t\n",
            "Epoch: [109][0/73]\tBatch Time 0.555 (0.555)\tData Time 0.360 (0.360)\tLoss 1.7750 (1.7750)\t\n",
            "Epoch: [110][0/73]\tBatch Time 1.024 (1.024)\tData Time 0.849 (0.849)\tLoss 2.7859 (2.7859)\t\n",
            "Epoch: [111][0/73]\tBatch Time 0.941 (0.941)\tData Time 0.705 (0.705)\tLoss 2.2563 (2.2563)\t\n",
            "Epoch: [112][0/73]\tBatch Time 1.003 (1.003)\tData Time 0.810 (0.810)\tLoss 2.3164 (2.3164)\t\n",
            "Epoch: [113][0/73]\tBatch Time 0.925 (0.925)\tData Time 0.749 (0.749)\tLoss 2.0055 (2.0055)\t\n",
            "Epoch: [114][0/73]\tBatch Time 0.607 (0.607)\tData Time 0.430 (0.430)\tLoss 1.9723 (1.9723)\t\n",
            "Epoch: [115][0/73]\tBatch Time 0.637 (0.637)\tData Time 0.457 (0.457)\tLoss 2.6402 (2.6402)\t\n",
            "Epoch: [116][0/73]\tBatch Time 0.752 (0.752)\tData Time 0.579 (0.579)\tLoss 2.4233 (2.4233)\t\n",
            "Epoch: [117][0/73]\tBatch Time 0.914 (0.914)\tData Time 0.741 (0.741)\tLoss 2.2996 (2.2996)\t\n",
            "Epoch: [118][0/73]\tBatch Time 0.740 (0.740)\tData Time 0.554 (0.554)\tLoss 2.5102 (2.5102)\t\n",
            "Epoch: [119][0/73]\tBatch Time 0.863 (0.863)\tData Time 0.666 (0.666)\tLoss 2.2641 (2.2641)\t\n",
            "Epoch: [120][0/73]\tBatch Time 0.655 (0.655)\tData Time 0.480 (0.480)\tLoss 2.7645 (2.7645)\t\n",
            "Epoch: [121][0/73]\tBatch Time 0.602 (0.602)\tData Time 0.429 (0.429)\tLoss 2.0526 (2.0526)\t\n",
            "Epoch: [122][0/73]\tBatch Time 0.760 (0.760)\tData Time 0.574 (0.574)\tLoss 1.8640 (1.8640)\t\n",
            "Epoch: [123][0/73]\tBatch Time 0.823 (0.823)\tData Time 0.653 (0.653)\tLoss 1.7415 (1.7415)\t\n",
            "Epoch: [124][0/73]\tBatch Time 1.092 (1.092)\tData Time 0.931 (0.931)\tLoss 1.8370 (1.8370)\t\n",
            "Epoch: [125][0/73]\tBatch Time 0.595 (0.595)\tData Time 0.416 (0.416)\tLoss 1.8788 (1.8788)\t\n",
            "Epoch: [126][0/73]\tBatch Time 0.798 (0.798)\tData Time 0.634 (0.634)\tLoss 1.9461 (1.9461)\t\n",
            "Epoch: [127][0/73]\tBatch Time 1.151 (1.151)\tData Time 0.974 (0.974)\tLoss 2.1011 (2.1011)\t\n",
            "Epoch: [128][0/73]\tBatch Time 1.110 (1.110)\tData Time 0.945 (0.945)\tLoss 2.1109 (2.1109)\t\n",
            "Epoch: [129][0/73]\tBatch Time 0.670 (0.670)\tData Time 0.504 (0.504)\tLoss 1.8806 (1.8806)\t\n",
            "Epoch: [130][0/73]\tBatch Time 0.711 (0.711)\tData Time 0.535 (0.535)\tLoss 2.0415 (2.0415)\t\n",
            "Epoch: [131][0/73]\tBatch Time 0.577 (0.577)\tData Time 0.400 (0.400)\tLoss 2.1921 (2.1921)\t\n",
            "Epoch: [132][0/73]\tBatch Time 0.737 (0.737)\tData Time 0.557 (0.557)\tLoss 2.2135 (2.2135)\t\n",
            "Epoch: [133][0/73]\tBatch Time 0.815 (0.815)\tData Time 0.631 (0.631)\tLoss 1.8805 (1.8805)\t\n",
            "Epoch: [134][0/73]\tBatch Time 0.685 (0.685)\tData Time 0.499 (0.499)\tLoss 2.5668 (2.5668)\t\n",
            "Epoch: [135][0/73]\tBatch Time 0.900 (0.900)\tData Time 0.706 (0.706)\tLoss 1.8668 (1.8668)\t\n",
            "Epoch: [136][0/73]\tBatch Time 0.533 (0.533)\tData Time 0.351 (0.351)\tLoss 2.1394 (2.1394)\t\n",
            "Epoch: [137][0/73]\tBatch Time 1.343 (1.343)\tData Time 1.156 (1.156)\tLoss 1.8955 (1.8955)\t\n",
            "Epoch: [138][0/73]\tBatch Time 0.444 (0.444)\tData Time 0.266 (0.266)\tLoss 3.3385 (3.3385)\t\n",
            "Epoch: [139][0/73]\tBatch Time 0.537 (0.537)\tData Time 0.359 (0.359)\tLoss 2.0197 (2.0197)\t\n",
            "Epoch: [140][0/73]\tBatch Time 1.079 (1.079)\tData Time 0.890 (0.890)\tLoss inf (inf)\t\n",
            "Epoch: [141][0/73]\tBatch Time 0.627 (0.627)\tData Time 0.446 (0.446)\tLoss 1.7260 (1.7260)\t\n",
            "Epoch: [142][0/73]\tBatch Time 1.015 (1.015)\tData Time 0.851 (0.851)\tLoss 2.1140 (2.1140)\t\n",
            "Epoch: [143][0/73]\tBatch Time 1.040 (1.040)\tData Time 0.858 (0.858)\tLoss 1.9134 (1.9134)\t\n",
            "Epoch: [144][0/73]\tBatch Time 0.852 (0.852)\tData Time 0.647 (0.647)\tLoss 1.8800 (1.8800)\t\n",
            "Epoch: [145][0/73]\tBatch Time 0.762 (0.762)\tData Time 0.577 (0.577)\tLoss 2.2825 (2.2825)\t\n",
            "Epoch: [146][0/73]\tBatch Time 1.274 (1.274)\tData Time 1.102 (1.102)\tLoss 2.3017 (2.3017)\t\n",
            "Epoch: [147][0/73]\tBatch Time 0.675 (0.675)\tData Time 0.491 (0.491)\tLoss 2.1396 (2.1396)\t\n",
            "Epoch: [148][0/73]\tBatch Time 0.693 (0.693)\tData Time 0.522 (0.522)\tLoss 1.9832 (1.9832)\t\n",
            "Epoch: [149][0/73]\tBatch Time 0.991 (0.991)\tData Time 0.803 (0.803)\tLoss 2.2176 (2.2176)\t\n",
            "Epoch: [150][0/73]\tBatch Time 0.834 (0.834)\tData Time 0.650 (0.650)\tLoss 2.3134 (2.3134)\t\n",
            "Epoch: [151][0/73]\tBatch Time 0.726 (0.726)\tData Time 0.544 (0.544)\tLoss 2.4180 (2.4180)\t\n",
            "Epoch: [152][0/73]\tBatch Time 1.017 (1.017)\tData Time 0.816 (0.816)\tLoss 1.6557 (1.6557)\t\n",
            "Epoch: [153][0/73]\tBatch Time 0.603 (0.603)\tData Time 0.420 (0.420)\tLoss 1.6718 (1.6718)\t\n",
            "Epoch: [154][0/73]\tBatch Time 0.853 (0.853)\tData Time 0.664 (0.664)\tLoss 2.3570 (2.3570)\t\n",
            "Epoch: [155][0/73]\tBatch Time 1.119 (1.119)\tData Time 0.942 (0.942)\tLoss 1.9429 (1.9429)\t\n",
            "Epoch: [156][0/73]\tBatch Time 0.580 (0.580)\tData Time 0.403 (0.403)\tLoss 2.1404 (2.1404)\t\n",
            "Epoch: [157][0/73]\tBatch Time 0.955 (0.955)\tData Time 0.779 (0.779)\tLoss 2.1650 (2.1650)\t\n",
            "Epoch: [158][0/73]\tBatch Time 0.460 (0.460)\tData Time 0.273 (0.273)\tLoss 1.9332 (1.9332)\t\n",
            "Epoch: [159][0/73]\tBatch Time 0.612 (0.612)\tData Time 0.423 (0.423)\tLoss 1.7458 (1.7458)\t\n",
            "Epoch: [160][0/73]\tBatch Time 0.811 (0.811)\tData Time 0.611 (0.611)\tLoss 1.8309 (1.8309)\t\n",
            "Epoch: [161][0/73]\tBatch Time 0.672 (0.672)\tData Time 0.480 (0.480)\tLoss 1.9033 (1.9033)\t\n",
            "Epoch: [162][0/73]\tBatch Time 0.546 (0.546)\tData Time 0.371 (0.371)\tLoss inf (inf)\t\n",
            "Epoch: [163][0/73]\tBatch Time 1.102 (1.102)\tData Time 0.911 (0.911)\tLoss 2.3339 (2.3339)\t\n",
            "Epoch: [164][0/73]\tBatch Time 0.752 (0.752)\tData Time 0.572 (0.572)\tLoss 1.6110 (1.6110)\t\n",
            "Epoch: [165][0/73]\tBatch Time 0.781 (0.781)\tData Time 0.594 (0.594)\tLoss 1.7420 (1.7420)\t\n",
            "DECAYING learning rate.\n",
            " The new LR is 0.000100\n",
            "\n",
            "Epoch: [166][0/73]\tBatch Time 0.662 (0.662)\tData Time 0.486 (0.486)\tLoss 1.7920 (1.7920)\t\n",
            "Epoch: [167][0/73]\tBatch Time 0.614 (0.614)\tData Time 0.425 (0.425)\tLoss 2.1878 (2.1878)\t\n",
            "Epoch: [168][0/73]\tBatch Time 1.675 (1.675)\tData Time 1.416 (1.416)\tLoss 1.7383 (1.7383)\t\n",
            "Epoch: [169][0/73]\tBatch Time 0.817 (0.817)\tData Time 0.635 (0.635)\tLoss 1.7267 (1.7267)\t\n",
            "Epoch: [170][0/73]\tBatch Time 1.154 (1.154)\tData Time 0.988 (0.988)\tLoss 1.7565 (1.7565)\t\n",
            "Epoch: [171][0/73]\tBatch Time 0.863 (0.863)\tData Time 0.697 (0.697)\tLoss 2.0645 (2.0645)\t\n",
            "Epoch: [172][0/73]\tBatch Time 0.726 (0.726)\tData Time 0.553 (0.553)\tLoss 1.7785 (1.7785)\t\n",
            "Epoch: [173][0/73]\tBatch Time 0.587 (0.587)\tData Time 0.386 (0.386)\tLoss 1.9166 (1.9166)\t\n",
            "Epoch: [174][0/73]\tBatch Time 1.196 (1.196)\tData Time 1.016 (1.016)\tLoss 2.6462 (2.6462)\t\n",
            "Epoch: [175][0/73]\tBatch Time 0.612 (0.612)\tData Time 0.422 (0.422)\tLoss 1.9897 (1.9897)\t\n",
            "Epoch: [176][0/73]\tBatch Time 0.744 (0.744)\tData Time 0.581 (0.581)\tLoss 1.6913 (1.6913)\t\n",
            "Epoch: [177][0/73]\tBatch Time 0.915 (0.915)\tData Time 0.735 (0.735)\tLoss 1.5396 (1.5396)\t\n",
            "Epoch: [178][0/73]\tBatch Time 0.578 (0.578)\tData Time 0.397 (0.397)\tLoss 1.9368 (1.9368)\t\n",
            "Epoch: [179][0/73]\tBatch Time 0.722 (0.722)\tData Time 0.556 (0.556)\tLoss 1.8313 (1.8313)\t\n",
            "Epoch: [180][0/73]\tBatch Time 0.952 (0.952)\tData Time 0.790 (0.790)\tLoss 1.5099 (1.5099)\t\n",
            "Epoch: [181][0/73]\tBatch Time 0.459 (0.459)\tData Time 0.270 (0.270)\tLoss 2.0071 (2.0071)\t\n",
            "Epoch: [182][0/73]\tBatch Time 0.843 (0.843)\tData Time 0.666 (0.666)\tLoss 1.6988 (1.6988)\t\n",
            "Epoch: [183][0/73]\tBatch Time 0.747 (0.747)\tData Time 0.563 (0.563)\tLoss 2.0723 (2.0723)\t\n",
            "Epoch: [184][0/73]\tBatch Time 0.615 (0.615)\tData Time 0.439 (0.439)\tLoss 1.6803 (1.6803)\t\n",
            "Epoch: [185][0/73]\tBatch Time 0.758 (0.758)\tData Time 0.578 (0.578)\tLoss 1.8503 (1.8503)\t\n",
            "Epoch: [186][0/73]\tBatch Time 0.663 (0.663)\tData Time 0.481 (0.481)\tLoss 2.1931 (2.1931)\t\n",
            "Epoch: [187][0/73]\tBatch Time 0.637 (0.637)\tData Time 0.450 (0.450)\tLoss 2.3109 (2.3109)\t\n",
            "Epoch: [188][0/73]\tBatch Time 0.604 (0.604)\tData Time 0.398 (0.398)\tLoss 2.1543 (2.1543)\t\n",
            "Epoch: [189][0/73]\tBatch Time 0.760 (0.760)\tData Time 0.582 (0.582)\tLoss 2.2437 (2.2437)\t\n",
            "Epoch: [190][0/73]\tBatch Time 0.483 (0.483)\tData Time 0.310 (0.310)\tLoss 2.3416 (2.3416)\t\n",
            "Epoch: [191][0/73]\tBatch Time 1.362 (1.362)\tData Time 1.126 (1.126)\tLoss 2.0214 (2.0214)\t\n",
            "Epoch: [192][0/73]\tBatch Time 0.961 (0.961)\tData Time 0.789 (0.789)\tLoss 2.2434 (2.2434)\t\n",
            "Epoch: [193][0/73]\tBatch Time 0.493 (0.493)\tData Time 0.314 (0.314)\tLoss 2.0794 (2.0794)\t\n",
            "Epoch: [194][0/73]\tBatch Time 0.851 (0.851)\tData Time 0.672 (0.672)\tLoss 2.3406 (2.3406)\t\n",
            "Epoch: [195][0/73]\tBatch Time 0.744 (0.744)\tData Time 0.562 (0.562)\tLoss 1.8374 (1.8374)\t\n",
            "Epoch: [196][0/73]\tBatch Time 0.729 (0.729)\tData Time 0.544 (0.544)\tLoss 2.7299 (2.7299)\t\n",
            "Epoch: [197][0/73]\tBatch Time 0.847 (0.847)\tData Time 0.663 (0.663)\tLoss 1.6673 (1.6673)\t\n",
            "Epoch: [198][0/73]\tBatch Time 0.565 (0.565)\tData Time 0.393 (0.393)\tLoss 1.5627 (1.5627)\t\n",
            "Epoch: [199][0/73]\tBatch Time 0.867 (0.867)\tData Time 0.685 (0.685)\tLoss 1.5331 (1.5331)\t\n",
            "Epoch: [200][0/73]\tBatch Time 0.716 (0.716)\tData Time 0.533 (0.533)\tLoss 1.8878 (1.8878)\t\n",
            "Epoch: [201][0/73]\tBatch Time 0.517 (0.517)\tData Time 0.335 (0.335)\tLoss 2.0861 (2.0861)\t\n",
            "Epoch: [202][0/73]\tBatch Time 0.592 (0.592)\tData Time 0.428 (0.428)\tLoss 1.7463 (1.7463)\t\n",
            "Epoch: [203][0/73]\tBatch Time 0.880 (0.880)\tData Time 0.682 (0.682)\tLoss 2.1755 (2.1755)\t\n",
            "Epoch: [204][0/73]\tBatch Time 0.630 (0.630)\tData Time 0.450 (0.450)\tLoss 1.9677 (1.9677)\t\n",
            "Epoch: [205][0/73]\tBatch Time 1.460 (1.460)\tData Time 1.233 (1.233)\tLoss 1.9257 (1.9257)\t\n",
            "Epoch: [206][0/73]\tBatch Time 0.618 (0.618)\tData Time 0.444 (0.444)\tLoss 1.6804 (1.6804)\t\n",
            "Epoch: [207][0/73]\tBatch Time 0.825 (0.825)\tData Time 0.638 (0.638)\tLoss 2.0805 (2.0805)\t\n",
            "Epoch: [208][0/73]\tBatch Time 0.632 (0.632)\tData Time 0.439 (0.439)\tLoss 2.1362 (2.1362)\t\n",
            "Epoch: [209][0/73]\tBatch Time 0.922 (0.922)\tData Time 0.744 (0.744)\tLoss 2.4145 (2.4145)\t\n",
            "Epoch: [210][0/73]\tBatch Time 0.654 (0.654)\tData Time 0.477 (0.477)\tLoss 1.9353 (1.9353)\t\n",
            "Epoch: [211][0/73]\tBatch Time 0.687 (0.687)\tData Time 0.514 (0.514)\tLoss 1.5501 (1.5501)\t\n",
            "Epoch: [212][0/73]\tBatch Time 0.818 (0.818)\tData Time 0.631 (0.631)\tLoss 1.7159 (1.7159)\t\n",
            "Epoch: [213][0/73]\tBatch Time 1.143 (1.143)\tData Time 0.954 (0.954)\tLoss 1.7104 (1.7104)\t\n",
            "Epoch: [214][0/73]\tBatch Time 0.551 (0.551)\tData Time 0.373 (0.373)\tLoss 2.0836 (2.0836)\t\n",
            "Epoch: [215][0/73]\tBatch Time 0.558 (0.558)\tData Time 0.367 (0.367)\tLoss 1.9170 (1.9170)\t\n",
            "Epoch: [216][0/73]\tBatch Time 0.877 (0.877)\tData Time 0.692 (0.692)\tLoss 1.5517 (1.5517)\t\n",
            "Epoch: [217][0/73]\tBatch Time 0.903 (0.903)\tData Time 0.731 (0.731)\tLoss 1.8507 (1.8507)\t\n",
            "Epoch: [218][0/73]\tBatch Time 0.862 (0.862)\tData Time 0.699 (0.699)\tLoss 1.9502 (1.9502)\t\n",
            "Epoch: [219][0/73]\tBatch Time 0.861 (0.861)\tData Time 0.669 (0.669)\tLoss 1.6664 (1.6664)\t\n",
            "Epoch: [220][0/73]\tBatch Time 1.000 (1.000)\tData Time 0.819 (0.819)\tLoss 1.6442 (1.6442)\t\n",
            "Epoch: [221][0/73]\tBatch Time 0.903 (0.903)\tData Time 0.726 (0.726)\tLoss 1.7629 (1.7629)\t\n",
            "DECAYING learning rate.\n",
            " The new LR is 0.000010\n",
            "\n",
            "Epoch: [222][0/73]\tBatch Time 1.053 (1.053)\tData Time 0.887 (0.887)\tLoss 2.0204 (2.0204)\t\n",
            "Epoch: [223][0/73]\tBatch Time 0.503 (0.503)\tData Time 0.312 (0.312)\tLoss 1.5791 (1.5791)\t\n",
            "Epoch: [224][0/73]\tBatch Time 0.978 (0.978)\tData Time 0.768 (0.768)\tLoss 2.1927 (2.1927)\t\n",
            "Epoch: [225][0/73]\tBatch Time 0.828 (0.828)\tData Time 0.649 (0.649)\tLoss 2.5228 (2.5228)\t\n",
            "Epoch: [226][0/73]\tBatch Time 0.709 (0.709)\tData Time 0.527 (0.527)\tLoss 1.4390 (1.4390)\t\n",
            "Epoch: [227][0/73]\tBatch Time 1.129 (1.129)\tData Time 0.957 (0.957)\tLoss 1.9944 (1.9944)\t\n",
            "Epoch: [228][0/73]\tBatch Time 0.820 (0.820)\tData Time 0.646 (0.646)\tLoss 1.6181 (1.6181)\t\n",
            "Epoch: [229][0/73]\tBatch Time 0.816 (0.816)\tData Time 0.628 (0.628)\tLoss 2.0488 (2.0488)\t\n",
            "Epoch: [230][0/73]\tBatch Time 0.836 (0.836)\tData Time 0.640 (0.640)\tLoss 2.3704 (2.3704)\t\n",
            "Epoch: [231][0/73]\tBatch Time 0.604 (0.604)\tData Time 0.423 (0.423)\tLoss 2.3211 (2.3211)\t\n",
            "Epoch: [232][0/73]\tBatch Time 0.779 (0.779)\tData Time 0.614 (0.614)\tLoss 1.7177 (1.7177)\t\n",
            "Epoch: [233][0/73]\tBatch Time 0.607 (0.607)\tData Time 0.412 (0.412)\tLoss 2.0487 (2.0487)\t\n",
            "Epoch: [234][0/73]\tBatch Time 0.589 (0.589)\tData Time 0.376 (0.376)\tLoss 1.7130 (1.7130)\t\n",
            "Epoch: [235][0/73]\tBatch Time 0.539 (0.539)\tData Time 0.367 (0.367)\tLoss 1.7251 (1.7251)\t\n",
            "Epoch: [236][0/73]\tBatch Time 1.696 (1.696)\tData Time 1.420 (1.420)\tLoss 1.9971 (1.9971)\t\n",
            "Epoch: [237][0/73]\tBatch Time 0.518 (0.518)\tData Time 0.340 (0.340)\tLoss 2.0254 (2.0254)\t\n",
            "Epoch: [238][0/73]\tBatch Time 0.880 (0.880)\tData Time 0.698 (0.698)\tLoss 1.9504 (1.9504)\t\n",
            "Epoch: [239][0/73]\tBatch Time 1.026 (1.026)\tData Time 0.843 (0.843)\tLoss 1.9301 (1.9301)\t\n",
            "Epoch: [240][0/73]\tBatch Time 0.768 (0.768)\tData Time 0.588 (0.588)\tLoss 1.9390 (1.9390)\t\n",
            "Epoch: [241][0/73]\tBatch Time 0.524 (0.524)\tData Time 0.338 (0.338)\tLoss 1.7940 (1.7940)\t\n",
            "Epoch: [242][0/73]\tBatch Time 0.992 (0.992)\tData Time 0.777 (0.777)\tLoss 1.6428 (1.6428)\t\n",
            "Epoch: [243][0/73]\tBatch Time 0.759 (0.759)\tData Time 0.567 (0.567)\tLoss 1.7550 (1.7550)\t\n",
            "Epoch: [244][0/73]\tBatch Time 0.965 (0.965)\tData Time 0.797 (0.797)\tLoss 1.8743 (1.8743)\t\n",
            "Epoch: [245][0/73]\tBatch Time 1.397 (1.397)\tData Time 1.138 (1.138)\tLoss 1.6255 (1.6255)\t\n",
            "Epoch: [246][0/73]\tBatch Time 0.654 (0.654)\tData Time 0.458 (0.458)\tLoss 2.0329 (2.0329)\t\n",
            "Epoch: [247][0/73]\tBatch Time 0.762 (0.762)\tData Time 0.582 (0.582)\tLoss 1.8576 (1.8576)\t\n",
            "Epoch: [248][0/73]\tBatch Time 0.837 (0.837)\tData Time 0.642 (0.642)\tLoss 2.2364 (2.2364)\t\n",
            "Epoch: [249][0/73]\tBatch Time 1.481 (1.481)\tData Time 1.315 (1.315)\tLoss 2.5304 (2.5304)\t\n",
            "Epoch: [250][0/73]\tBatch Time 0.699 (0.699)\tData Time 0.518 (0.518)\tLoss 1.6375 (1.6375)\t\n",
            "Epoch: [251][0/73]\tBatch Time 0.531 (0.531)\tData Time 0.352 (0.352)\tLoss 1.7542 (1.7542)\t\n",
            "Epoch: [252][0/73]\tBatch Time 0.667 (0.667)\tData Time 0.473 (0.473)\tLoss 1.6630 (1.6630)\t\n",
            "Epoch: [253][0/73]\tBatch Time 0.903 (0.903)\tData Time 0.728 (0.728)\tLoss 1.6028 (1.6028)\t\n",
            "Epoch: [254][0/73]\tBatch Time 0.672 (0.672)\tData Time 0.478 (0.478)\tLoss 1.8207 (1.8207)\t\n",
            "Epoch: [255][0/73]\tBatch Time 0.557 (0.557)\tData Time 0.380 (0.380)\tLoss 2.0318 (2.0318)\t\n",
            "Epoch: [256][0/73]\tBatch Time 0.792 (0.792)\tData Time 0.598 (0.598)\tLoss 1.6409 (1.6409)\t\n",
            "Epoch: [257][0/73]\tBatch Time 0.868 (0.868)\tData Time 0.687 (0.687)\tLoss 2.3567 (2.3567)\t\n",
            "Epoch: [258][0/73]\tBatch Time 1.035 (1.035)\tData Time 0.864 (0.864)\tLoss 2.0813 (2.0813)\t\n",
            "Epoch: [259][0/73]\tBatch Time 1.186 (1.186)\tData Time 0.972 (0.972)\tLoss 1.8966 (1.8966)\t\n",
            "Epoch: [260][0/73]\tBatch Time 0.600 (0.600)\tData Time 0.414 (0.414)\tLoss 1.8754 (1.8754)\t\n",
            "Epoch: [261][0/73]\tBatch Time 0.926 (0.926)\tData Time 0.751 (0.751)\tLoss 1.6575 (1.6575)\t\n",
            "Epoch: [262][0/73]\tBatch Time 1.239 (1.239)\tData Time 1.049 (1.049)\tLoss 1.4235 (1.4235)\t\n",
            "Epoch: [263][0/73]\tBatch Time 0.657 (0.657)\tData Time 0.478 (0.478)\tLoss 1.6595 (1.6595)\t\n",
            "Epoch: [264][0/73]\tBatch Time 0.598 (0.598)\tData Time 0.420 (0.420)\tLoss 1.5294 (1.5294)\t\n",
            "Epoch: [265][0/73]\tBatch Time 1.228 (1.228)\tData Time 1.011 (1.011)\tLoss 1.4720 (1.4720)\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохранение модели."
      ],
      "metadata": {
        "id": "C-xhvfJhPpsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "6noPkt-v8Ned"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка модели."
      ],
      "metadata": {
        "id": "l9VaOSu-Ps87"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SSD300(n_classes=4)\n",
        "model.load_state_dict(torch.load('model.pth', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qutKIVWnGMK9",
        "outputId": "1ee09285-6d47-4600-814d-0b3630ad4439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded base model.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation."
      ],
      "metadata": {
        "id": "9StwvYj-Pxlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties):\n",
        "    \"\"\"\n",
        "    Calculate the Mean Average Precision (mAP) of detected objects.\n",
        "\n",
        "    See https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173 for an explanation\n",
        "\n",
        "    :param det_boxes: list of tensors, one tensor for each image containing detected objects' bounding boxes\n",
        "    :param det_labels: list of tensors, one tensor for each image containing detected objects' labels\n",
        "    :param det_scores: list of tensors, one tensor for each image containing detected objects' labels' scores\n",
        "    :param true_boxes: list of tensors, one tensor for each image containing actual objects' bounding boxes\n",
        "    :param true_labels: list of tensors, one tensor for each image containing actual objects' labels\n",
        "    :param true_difficulties: list of tensors, one tensor for each image containing actual objects' difficulty (0 or 1)\n",
        "    :return: list of average precisions for all classes, mean average precision (mAP)\n",
        "    \"\"\"\n",
        "    assert len(det_boxes) == len(det_labels) == len(det_scores) == len(true_boxes) == len(\n",
        "        true_labels) == len(\n",
        "        true_difficulties)  # these are all lists of tensors of the same length, i.e. number of images\n",
        "    n_classes = len(label_map)\n",
        "\n",
        "    # Store all (true) objects in a single continuous tensor while keeping track of the image it is from\n",
        "    true_images = list()\n",
        "    for i in range(len(true_labels)):\n",
        "        true_images.extend([i] * true_labels[i].size(0))\n",
        "    true_images = torch.LongTensor(true_images).to(\n",
        "        device)  # (n_objects), n_objects is the total no. of objects across all images\n",
        "    true_boxes = torch.cat(true_boxes, dim=0)  # (n_objects, 4)\n",
        "    true_labels = torch.cat(true_labels, dim=0)  # (n_objects)\n",
        "    true_difficulties = torch.cat(true_difficulties, dim=0)  # (n_objects)\n",
        "\n",
        "    assert true_images.size(0) == true_boxes.size(0) == true_labels.size(0)\n",
        "\n",
        "    # Store all detections in a single continuous tensor while keeping track of the image it is from\n",
        "    det_images = list()\n",
        "    for i in range(len(det_labels)):\n",
        "        det_images.extend([i] * det_labels[i].size(0))\n",
        "    det_images = torch.LongTensor(det_images).to(device)  # (n_detections)\n",
        "    det_boxes = torch.cat(det_boxes, dim=0)  # (n_detections, 4)\n",
        "    det_labels = torch.cat(det_labels, dim=0)  # (n_detections)\n",
        "    det_scores = torch.cat(det_scores, dim=0)  # (n_detections)\n",
        "\n",
        "    assert det_images.size(0) == det_boxes.size(0) == det_labels.size(0) == det_scores.size(0)\n",
        "\n",
        "    # Calculate APs for each class (except background)\n",
        "    average_precisions = torch.zeros((n_classes - 1), dtype=torch.float)  # (n_classes - 1)\n",
        "    for c in range(1, n_classes):\n",
        "        # Extract only objects with this class\n",
        "        true_class_images = true_images[true_labels == c].to(device)  # (n_class_objects)\n",
        "        true_class_boxes = true_boxes[true_labels == c]  # (n_class_objects, 4)\n",
        "        true_class_difficulties = true_difficulties[true_labels == c]  # (n_class_objects)\n",
        "        n_easy_class_objects = (1 - true_class_difficulties).sum().item()  # ignore difficult objects\n",
        "\n",
        "        # Keep track of which true objects with this class have already been 'detected'\n",
        "        # So far, none\n",
        "        true_class_boxes_detected = torch.zeros((true_class_difficulties.size(0)), dtype=torch.uint8).to(\n",
        "            device)  # (n_class_objects)\n",
        "\n",
        "        # Extract only detections with this class\n",
        "        det_class_images = det_images[det_labels == c]  # (n_class_detections)\n",
        "        det_class_boxes = det_boxes[det_labels == c]  # (n_class_detections, 4)\n",
        "        det_class_scores = det_scores[det_labels == c]  # (n_class_detections)\n",
        "        n_class_detections = det_class_boxes.size(0)\n",
        "        if n_class_detections == 0:\n",
        "            continue\n",
        "\n",
        "        # Sort detections in decreasing order of confidence/scores\n",
        "        det_class_scores, sort_ind = torch.sort(det_class_scores, dim=0, descending=True)  # (n_class_detections)\n",
        "        det_class_images = det_class_images[sort_ind]  # (n_class_detections)\n",
        "        det_class_boxes = det_class_boxes[sort_ind]  # (n_class_detections, 4)\n",
        "\n",
        "        # In the order of decreasing scores, check if true or false positive\n",
        "        true_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
        "        false_positives = torch.zeros((n_class_detections), dtype=torch.float).to(device)  # (n_class_detections)\n",
        "        for d in range(n_class_detections):\n",
        "            this_detection_box = det_class_boxes[d].unsqueeze(0)  # (1, 4)\n",
        "            this_image = det_class_images[d]  # (), scalar\n",
        "\n",
        "            # Find objects in the same image with this class, their difficulties, and whether they have been detected before\n",
        "            object_boxes = true_class_boxes[true_class_images == this_image]  # (n_class_objects_in_img)\n",
        "            object_difficulties = true_class_difficulties[true_class_images == this_image]  # (n_class_objects_in_img)\n",
        "            # If no such object in this image, then the detection is a false positive\n",
        "            if object_boxes.size(0) == 0:\n",
        "                false_positives[d] = 1\n",
        "                continue\n",
        "\n",
        "            # Find maximum overlap of this detection with objects in this image of this class\n",
        "            overlaps = find_jaccard_overlap(this_detection_box, object_boxes)  # (1, n_class_objects_in_img)\n",
        "            max_overlap, ind = torch.max(overlaps.squeeze(0), dim=0)  # (), () - scalars\n",
        "\n",
        "            # 'ind' is the index of the object in these image-level tensors 'object_boxes', 'object_difficulties'\n",
        "            # In the original class-level tensors 'true_class_boxes', etc., 'ind' corresponds to object with index...\n",
        "            original_ind = torch.LongTensor(range(true_class_boxes.size(0)))[true_class_images == this_image][ind]\n",
        "            # We need 'original_ind' to update 'true_class_boxes_detected'\n",
        "\n",
        "            # If the maximum overlap is greater than the threshold of 0.5, it's a match\n",
        "            if max_overlap.item() > 0.5:\n",
        "                # If the object it matched with is 'difficult', ignore it\n",
        "                if object_difficulties[ind] == 0:\n",
        "                    # If this object has already not been detected, it's a true positive\n",
        "                    if true_class_boxes_detected[original_ind] == 0:\n",
        "                        true_positives[d] = 1\n",
        "                        true_class_boxes_detected[original_ind] = 1  # this object has now been detected/accounted for\n",
        "                    # Otherwise, it's a false positive (since this object is already accounted for)\n",
        "                    else:\n",
        "                        false_positives[d] = 1\n",
        "            # Otherwise, the detection occurs in a different location than the actual object, and is a false positive\n",
        "            else:\n",
        "                false_positives[d] = 1\n",
        "\n",
        "        # Compute cumulative precision and recall at each detection in the order of decreasing scores\n",
        "        cumul_true_positives = torch.cumsum(true_positives, dim=0)  # (n_class_detections)\n",
        "        cumul_false_positives = torch.cumsum(false_positives, dim=0)  # (n_class_detections)\n",
        "        cumul_precision = cumul_true_positives / (\n",
        "                cumul_true_positives + cumul_false_positives + 1e-10)  # (n_class_detections)\n",
        "        cumul_recall = cumul_true_positives / n_easy_class_objects  # (n_class_detections)\n",
        "\n",
        "        # Find the mean of the maximum of the precisions corresponding to recalls above the threshold 't'\n",
        "        recall_thresholds = torch.arange(start=0, end=1.1, step=.1).tolist()  # (11)\n",
        "        precisions = torch.zeros((len(recall_thresholds)), dtype=torch.float).to(device)  # (11)\n",
        "        for i, t in enumerate(recall_thresholds):\n",
        "            recalls_above_t = cumul_recall >= t\n",
        "            if recalls_above_t.any():\n",
        "                precisions[i] = cumul_precision[recalls_above_t].max()\n",
        "            else:\n",
        "                precisions[i] = 0.\n",
        "        average_precisions[c - 1] = precisions.mean()  # c is in [1, n_classes - 1]\n",
        "\n",
        "    # Calculate Mean Average Precision (mAP)\n",
        "    mean_average_precision = average_precisions.mean().item()\n",
        "\n",
        "    # Keep class-wise average precisions in a dictionary\n",
        "    average_precisions = {rev_label_map[c + 1]: v for c, v in enumerate(average_precisions.tolist())}\n",
        "\n",
        "    return average_precisions, mean_average_precision"
      ],
      "metadata": {
        "id": "zTk4v8RRmjyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from pprint import PrettyPrinter\n",
        "\n",
        "# Good formatting when printing the APs for each class and mAP\n",
        "pp = PrettyPrinter()\n",
        "\n",
        "# Parameters\n",
        "data_folder = 'BCCD_Dataset'\n",
        "keep_difficult = True  # difficult ground truth objects must always be considered in mAP calculation, because these objects DO exist!\n",
        "batch_size = 4\n",
        "workers = 2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# checkpoint = './checkpoint_ssd300.pth.tar'\n",
        "\n",
        "# Load model checkpoint that is to be evaluated\n",
        "# checkpoint = torch.load(checkpoint)\n",
        "# model = checkpoint['model']\n",
        "# model = model.to(device)\n",
        "\n",
        "# Switch to eval mode\n",
        "model.eval()\n",
        "\n",
        "# Load test data\n",
        "test_dataset = BCCD_Dataset(data_folder,\n",
        "                                split='test',\n",
        "                                keep_difficult=keep_difficult)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                          collate_fn=test_dataset.collate_fn, num_workers=workers, pin_memory=True)\n",
        "\n",
        "\n",
        "def evaluate(test_loader, model):\n",
        "    \"\"\"\n",
        "    Evaluate.\n",
        "\n",
        "    :param test_loader: DataLoader for test data\n",
        "    :param model: model\n",
        "    \"\"\"\n",
        "\n",
        "    # Make sure it's in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Lists to store detected and true boxes, labels, scores\n",
        "    det_boxes = list()\n",
        "    det_labels = list()\n",
        "    det_scores = list()\n",
        "    true_boxes = list()\n",
        "    true_labels = list()\n",
        "    true_difficulties = list()  # it is necessary to know which objects are 'difficult', see 'calculate_mAP' in utils.py\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (images, boxes, labels, difficulties) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
        "            images = images.to(device)  # (N, 3, 300, 300)\n",
        "\n",
        "            # Forward prop.\n",
        "            predicted_locs, predicted_scores = model(images)\n",
        "\n",
        "            # Detect objects in SSD output\n",
        "            det_boxes_batch, det_labels_batch, det_scores_batch = model.detect_objects(predicted_locs, predicted_scores,\n",
        "                                                                                       min_score=0.01, max_overlap=0.45,\n",
        "                                                                                       top_k=200)\n",
        "            # Evaluation MUST be at min_score=0.01, max_overlap=0.45, top_k=200 for fair comparision with the paper's results and other repos\n",
        "\n",
        "            # Store this batch's results for mAP calculation\n",
        "            boxes = [b.to(device) for b in boxes]\n",
        "            labels = [l.to(device) for l in labels]\n",
        "            difficulties = [d.to(device) for d in difficulties]\n",
        "\n",
        "            det_boxes.extend(det_boxes_batch)\n",
        "            det_labels.extend(det_labels_batch)\n",
        "            det_scores.extend(det_scores_batch)\n",
        "            true_boxes.extend(boxes)\n",
        "            true_labels.extend(labels)\n",
        "            true_difficulties.extend(difficulties)\n",
        "\n",
        "        # Calculate mAP\n",
        "        APs, mAP = calculate_mAP(det_boxes, det_labels, det_scores, true_boxes, true_labels, true_difficulties)\n",
        "\n",
        "    # Print AP for each class\n",
        "    pp.pprint(APs)\n",
        "\n",
        "    print('\\nMean Average Precision (mAP): %.3f' % mAP)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    evaluate(test_loader, model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8NB25zXShR5",
        "outputId": "44aa4ae9-fb14-496d-b0f8-992a170c6327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   0%|          | 0/18 [00:00<?, ?it/s]<ipython-input-12-b47224018847>:500: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  image_boxes.append(class_decoded_locs[1 - suppress])\n",
            "<ipython-input-12-b47224018847>:502: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  image_scores.append(class_scores[1 - suppress])\n",
            "Evaluating: 100%|██████████| 18/18 [01:53<00:00,  6.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'platelets': 0.7941421270370483,\n",
            " 'rbc': 0.8249870538711548,\n",
            " 'wbc': 0.983408510684967}\n",
            "\n",
            "Mean Average Precision (mAP): 0.868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**На тестовой выборке метрика Precision модели по детекции клеток крови составила 0,868.**"
      ],
      "metadata": {
        "id": "Z2hl9TPLSy0U"
      }
    }
  ]
}